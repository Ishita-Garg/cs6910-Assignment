{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ques 10 assignment 1",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEhgNreByvTJ"
      },
      "source": [
        "import random\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from keras.datasets import fashion_mnist\r\n",
        "from sklearn.model_selection  import train_test_split\r\n",
        "import itertools\r\n",
        "import math\r\n",
        "!pip install wandb\r\n",
        "import wandb\r\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnOMsHbSJi5a",
        "outputId": "abdd89bc-1d9c-4a61-9da9-eb7a8773e25b"
      },
      "source": [
        "wandb.login()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdebugger30\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5A9RIg4uhXWN"
      },
      "source": [
        ""
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lUc5ltEQcM_"
      },
      "source": [
        "from sklearn.datasets import fetch_openml\r\n",
        "x,y = fetch_openml('mnist_784', version=1, return_X_y = True)\r\n",
        "train_images, test_images, train_labels, test_labels  = train_test_split(x,y,test_size=10000,random_state = 42)\r\n",
        "train_images, val_images, train_labels, val_labels  = train_test_split(train_images,train_labels,test_size=0.1,random_state = 42)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpHBk1c_Q8lx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fMVYaXPdYql"
      },
      "source": [
        "traind = train_images\r\n",
        "mean = traind.mean(axis=0)\r\n",
        "centerd = traind -  mean\r\n",
        "max = centerd.max(axis=0)\r\n",
        "normalized = centerd/np.max(max)\r\n",
        "\r\n",
        "mean = val_images.mean(axis=0)\r\n",
        "centerd = val_images -  mean\r\n",
        "max = centerd.max(axis=0)\r\n",
        "val_images = centerd/np.max(max)\r\n",
        "\r\n",
        "mean = test_images.mean(axis=0)\r\n",
        "centerd = test_images -  mean\r\n",
        "max = centerd.max(axis=0)\r\n",
        "test_images = centerd/np.max(max)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjyCexR0Iszw"
      },
      "source": [
        "class NeuralNet:\r\n",
        "    def __init__(self,Size_of_Input, Number_of_Neuron_each_Layer, Number_of_Layers, activation_function, typeOfInit, L2reg_const = 0):\r\n",
        "        self.activation_function = activation_function\r\n",
        "        self.Size_of_Input = Size_of_Input\r\n",
        "        self.Number_of_Layers = Number_of_Layers\r\n",
        "        self.Number_of_Neuron_each_Layer = Number_of_Neuron_each_Layer\r\n",
        "        self.L2reg_const = L2reg_const\r\n",
        "        self.W,self.b = self.initializer(typeOfInit)\r\n",
        "\r\n",
        "\r\n",
        "    \r\n",
        "    def initializer(self, init):        \r\n",
        "        W = []\r\n",
        "        b = []\r\n",
        "        if init == 'random':\r\n",
        "            W.append(np.random.randn(self.Number_of_Neuron_each_Layer[0], self.Size_of_Input))\r\n",
        "            for i in range(1,self.Number_of_Layers):\r\n",
        "                W.append(np.random.randn(self.Number_of_Neuron_each_Layer[i],self.Number_of_Neuron_each_Layer[i-1]))\r\n",
        "\r\n",
        "            for i in range(self.Number_of_Layers):\r\n",
        "                b.append(np.random.rand(self.Number_of_Neuron_each_Layer[i]))\r\n",
        "        elif(init == 'xavier'):\r\n",
        "            W.append(np.random.normal(0,math.sqrt(2/(self.Number_of_Neuron_each_Layer[0] + self.Size_of_Input)), (self.Number_of_Neuron_each_Layer[0], self.Size_of_Input)))\r\n",
        "            for i in range(1,self.Number_of_Layers):\r\n",
        "                W.append(np.random.normal(0, math.sqrt(2/(self.Number_of_Neuron_each_Layer[i] + self.Number_of_Neuron_each_Layer[i-1])),(self.Number_of_Neuron_each_Layer[i],self.Number_of_Neuron_each_Layer[i-1])))\r\n",
        "\r\n",
        "            for i in range(self.Number_of_Layers):\r\n",
        "                b.append(np.random.rand(self.Number_of_Neuron_each_Layer[i]))\r\n",
        "        return W,b\r\n",
        "\r\n",
        "\r\n",
        "    def activation(self, Z):\r\n",
        "        if self.activation_function == 'ReLU':\r\n",
        "            return self.ReLU(Z)\r\n",
        "        elif self.activation_function == 'tanh':\r\n",
        "            return self.tanh(Z)\r\n",
        "        elif self.activation_function == 'sigmoid':\r\n",
        "            return self.sigmoid(Z)\r\n",
        "\r\n",
        "\r\n",
        "    def activation_derivative(self,Z):\r\n",
        "        if self.activation_function == 'ReLU':\r\n",
        "            return self.ReLU_derivative(Z)\r\n",
        "        elif self.activation_function == 'tanh':\r\n",
        "            return self.tanh_derivative(Z)\r\n",
        "        elif self.activation_function == 'sigmoid':\r\n",
        "            return self.sigmoid_derivative(Z)\r\n",
        "\r\n",
        "    def ReLU(self,Z):\r\n",
        "        return np.maximum(0,Z)\r\n",
        "\r\n",
        "    def ReLU_derivative(self,Z):\r\n",
        "        return [1 if x>0 else 0 for x in Z]\r\n",
        "\r\n",
        "    def tanh(self, Z):\r\n",
        "        return np.array([((np.exp(x) - np.exp(-x))/((np.exp(x) + np.exp(-x)))) for x in Z])\r\n",
        "                 \r\n",
        "    def tanh_derivative(self, Z):\r\n",
        "        return np.array(1 - self.tanh(Z)**2)\r\n",
        "                 \r\n",
        "    def sigmoid_derivative(self,Z):\r\n",
        "        return self.sigmoid(Z)*(1-self.sigmoid(Z))\r\n",
        "\r\n",
        "    def sigmoid(self,x):\r\n",
        "        return np.where(x>=0, 1/(1+np.exp(-x)), np.exp(x)/(1+np.exp(x)))\r\n",
        "    \r\n",
        "    def softmax_function(self,Z):\r\n",
        "            Z = Z - Z.max()\r\n",
        "            return (np.exp(Z)/np.sum(np.exp(Z),axis=0))\r\n",
        "\r\n",
        "    def forward_propagation(self,Input):\r\n",
        "        A = []\r\n",
        "        H = []\r\n",
        "        Input = np.array(Input)\r\n",
        "        A.append(self.W[0].dot(Input) + self.b[0])\r\n",
        "        for i in range(1, self.Number_of_Layers):\r\n",
        "            H.append(self.activation(A[-1]))\r\n",
        "            A.append(self.W[i].dot(H[-1]) + self.b[i])\r\n",
        "        y_hat = self.softmax_function(A[-1])\r\n",
        "        return A, H, y_hat\r\n",
        "\r\n",
        "    def backward_propagation(self, A, H, y_hat, y, Input):\r\n",
        "        delA = []\r\n",
        "        delH = []\r\n",
        "        delW = []\r\n",
        "        delb = []\r\n",
        "        Input = np.array(Input)\r\n",
        "        H.insert(0,Input)\r\n",
        "        ey = np.zeros(self.Number_of_Neuron_each_Layer[-1])\r\n",
        "        ey[y] = 1\r\n",
        "        delA.append(np.array(-(ey - y_hat)))\r\n",
        "        for i in range(self.Number_of_Layers-1,-1,-1):\r\n",
        "            delW.insert(0,delA[-1].reshape(delA[-1].shape[0],1).dot(H[i].reshape(H[i].shape[0],1).T) + self.L2reg_const*self.W[i])\r\n",
        "            delb.insert(0,delA[-1])\r\n",
        "            delH.append(self.W[i].T.dot(delA[-1]))\r\n",
        "            if i-1>=0:\r\n",
        "                delA.append(np.multiply(delH[-1], self.activation_derivative(A[i-1])))\r\n",
        "        return delW,delb\r\n",
        "    \r\n",
        "    \r\n",
        "    def initialize(self, Size_of_Input,Number_of_Layers,Number_of_Neuron_each_Layer):\r\n",
        "        W, b = [], []\r\n",
        "        W.append(np.zeros((Number_of_Neuron_each_Layer[0], Size_of_Input)))\r\n",
        "        for i in range(1,Number_of_Layers):\r\n",
        "            W.append(np.zeros((Number_of_Neuron_each_Layer[i],Number_of_Neuron_each_Layer[i-1])))\r\n",
        "        for i in range(Number_of_Layers):\r\n",
        "            b.append(np.zeros(Number_of_Neuron_each_Layer[i]))\r\n",
        "            \r\n",
        "        return W, b\r\n",
        "    \r\n",
        "    def optimize(self, X, Y, val_images,val_labels,optimizer, learning_rate, max_epochs,batch_size):\r\n",
        "        if optimizer == 'momentum':\r\n",
        "          self.momentum_gradient_descent(X, Y, val_images,val_labels, learning_rate, max_epochs,batch_size)\r\n",
        "        elif optimizer == 'nag':\r\n",
        "          self.nesterov_accelerated_gradient_descent(X, Y, val_images,val_labels, learning_rate, max_epochs,batch_size)\r\n",
        "        elif optimizer == 'nadam':\r\n",
        "          self.nadam(X, Y, val_images,val_labels, learning_rate, max_epochs,batch_size)\r\n",
        "\r\n",
        "\r\n",
        "    def momentum_gradient_descent(self,X, Y, val_images,val_labels, learning_rate, max_epochs,batch_size, gamma = 0.6):\r\n",
        "        final_accuracy = 0\r\n",
        "        val_accuracy = 0\r\n",
        "        updateW, updateb = self.initialize(self.Size_of_Input,self.Number_of_Layers,self.Number_of_Neuron_each_Layer)\r\n",
        "\r\n",
        "        for j in range(max_epochs):\r\n",
        "            correct = 0\r\n",
        "\r\n",
        "            delW, delb = self.initialize(self.Size_of_Input,self.Number_of_Layers,self.Number_of_Neuron_each_Layer)\r\n",
        "\r\n",
        "            for i in range(X.shape[0]):\r\n",
        "                A,H,y_hat = self.forward_propagation(X[i])\r\n",
        "\r\n",
        "                \r\n",
        "                w,b = self.backward_propagation(A,H,y_hat,int(Y[i]),X[i])\r\n",
        "\r\n",
        "                for k in range(self.Number_of_Layers):\r\n",
        "                    delW[k] += w[k]\r\n",
        "                    delb[k] += b[k]\r\n",
        "\r\n",
        "                for k in range(self.Number_of_Layers):\r\n",
        "                    updateW[k] = gamma*updateW[k] + learning_rate*delW[k]   \r\n",
        "                    updateb[k] = gamma*updateb[k] + learning_rate*delb[k]\r\n",
        "\r\n",
        "\r\n",
        "                \r\n",
        "                if  (i%batch_size == 0 and i!=0) or i==X.shape[0]-1:\r\n",
        "                    delW, delb = self.initialize(self.Size_of_Input,self.Number_of_Layers,self.Number_of_Neuron_each_Layer)\r\n",
        "                    for k in range(self.Number_of_Layers):\r\n",
        "                        self.W[k] += -updateW[k]  \r\n",
        "                        self.b[k] += -updateb[k]\r\n",
        "\r\n",
        "                if(np.argmax(y_hat) == int(Y[i])):\r\n",
        "                    correct +=1\r\n",
        "\r\n",
        "                \r\n",
        "            accuracy = correct/X.shape[0]*100\r\n",
        "            v_error, v_accruracy = self.val_loss_and_accuracy(val_images, val_labels)\r\n",
        "            final_accuracy = accuracy\r\n",
        "            val_accuracy = v_accruracy\r\n",
        "            print(\"Train accuracy after {} epoch : \", final_accuracy)\r\n",
        "            print(\"Validation accuracy after {} epoch : \", val_accuracy)\r\n",
        "\r\n",
        "    def nesterov_accelerated_gradient_descent(self, X, Y, val_images,val_labels, learning_rate, max_epochs,batch_size, gamma = 0.5):\r\n",
        "        updateW, updateb = self.initialize( self.Size_of_Input, self.Number_of_Layers,self.Number_of_Neuron_each_Layer)\r\n",
        "        lookaheadW, lookaheadb = self.initialize( self.Size_of_Input, self.Number_of_Layers,self.Number_of_Neuron_each_Layer)\r\n",
        "        thetaW, thetab = self.initialize( self.Size_of_Input, self.Number_of_Layers,self.Number_of_Neuron_each_Layer)\r\n",
        "\r\n",
        "        for j in range(max_epochs):\r\n",
        "            correct = 0\r\n",
        "            error = 0\r\n",
        "\r\n",
        "            delW, delb = self.initialize( self.Size_of_Input, self.Number_of_Layers,self.Number_of_Neuron_each_Layer)\r\n",
        "                \r\n",
        "            for k in range( self.Number_of_Layers):\r\n",
        "                thetaW[k] = self.W[k]\r\n",
        "                thetab[k] = self.b[k]\r\n",
        "\r\n",
        "            for k in range( self.Number_of_Layers):\r\n",
        "                lookaheadW[k] = thetaW[k] - gamma*updateW[k]    \r\n",
        "                lookaheadb[k] = thetab[k] - gamma*updateb[k]\r\n",
        "                self.W[k] = lookaheadW[k]\r\n",
        "                self.b[k] = lookaheadb[k]\r\n",
        "\r\n",
        "            \r\n",
        "            for i in range(X.shape[0]):\r\n",
        "                A,H,y_hat = self.forward_propagation(X[i])\r\n",
        "                \r\n",
        "                w,b = self.backward_propagation(A,H,y_hat,int(Y[i]),X[i])\r\n",
        "\r\n",
        "                for k in range( self.Number_of_Layers):\r\n",
        "                    delW[k] += w[k]\r\n",
        "                    delb[k] += b[k]\r\n",
        "\r\n",
        "                for k in range( self.Number_of_Layers):\r\n",
        "                    updateW[k] =  gamma*updateW[k] + learning_rate*delW[k]   \r\n",
        "                    updateb[k] = gamma*updateb[k] + learning_rate*delb[k]\r\n",
        "\r\n",
        "                \r\n",
        "                if  (i%batch_size == 0 and i!=0) or i==X.shape[0]-1:\r\n",
        "                    delW, delb = self.initialize(self.Size_of_Input,self.Number_of_Layers,self.Number_of_Neuron_each_Layer)\r\n",
        "                    for k in range(self.Number_of_Layers):\r\n",
        "                        self.W[k] += -updateW[k]  \r\n",
        "                        self.b[k] += -updateb[k]\r\n",
        "                \r\n",
        "                if(np.argmax(y_hat) == int(Y[i])):\r\n",
        "                    correct +=1\r\n",
        "            \r\n",
        "            accuracy = correct/X.shape[0]*100\r\n",
        "            v_error, v_accuracy = self.val_loss_and_accuracy(val_images, val_labels)\r\n",
        "            final_accuracy = accuracy\r\n",
        "            val_accuracy = v_accuracy\r\n",
        "            print(\"Train accuracy after {} epoch : \", final_accuracy)\r\n",
        "            print(\"Validation accuracy after {} epoch : \", val_accuracy)\r\n",
        "\r\n",
        "    \r\n",
        "    def nadam(self, X, Y, val_images,val_labels, learning_rate, max_epochs,batch_size, beta1 = 0.89,beta2 = 0.989,epsilon = 1e-8):\r\n",
        "        m_W, m_b = self.initialize(self.Size_of_Input,self.Number_of_Layers,self.Number_of_Neuron_each_Layer)\r\n",
        "        m_hat_W, m_hat_b = self.initialize(self.Size_of_Input,self.Number_of_Layers,self.Number_of_Neuron_each_Layer)\r\n",
        "        v_W, v_b = self.initialize(self.Size_of_Input,self.Number_of_Layers,self.Number_of_Neuron_each_Layer)\r\n",
        "        v_hat_W, v_hat_b = self.initialize(self.Size_of_Input,self.Number_of_Layers,self.Number_of_Neuron_each_Layer)\r\n",
        "        final_accuracy = 0\r\n",
        "        val_accuracy = 0\r\n",
        "        for j in range(max_epochs):\r\n",
        "            correct = 0\r\n",
        "\r\n",
        "            delW, delb = self.initialize(self.Size_of_Input,self.Number_of_Layers,self.Number_of_Neuron_each_Layer)\r\n",
        "        \r\n",
        "            for i in range(X.shape[0]):\r\n",
        "                A,H,y_hat = self.forward_propagation(X[i])\r\n",
        "                \r\n",
        "\r\n",
        "                w,b = self.backward_propagation(A,H,y_hat,int(Y[i]),X[i])\r\n",
        "\r\n",
        "                for k in range(self.Number_of_Layers):\r\n",
        "                    delW[k] += w[k]\r\n",
        "                    delb[k] += b[k]\r\n",
        "                \r\n",
        "                if(np.argmax(y_hat) == int(Y[i])):\r\n",
        "                    correct +=1\r\n",
        "\r\n",
        "                if  (i%batch_size == 0 and i!=0) or i==X.shape[0]-1:\r\n",
        "                    for k in range(self.Number_of_Layers):\r\n",
        "                        v_W[k] =  beta2*v_W[k] + (1-beta2)*delW[k]**2\r\n",
        "                        v_b[k] = beta2*v_b[k] + (1-beta2)*delb[k]**2\r\n",
        "                        m_W[k] = beta1*m_W[k] + (1-beta1)*delW[k]\r\n",
        "                        m_b[k] = beta1*m_b[k] + (1-beta1)*delb[k]\r\n",
        "                        m_hat_W[k] = m_W[k]/(math.pow(beta1,j))\r\n",
        "                        m_hat_b[k] = m_b[k]/(math.pow(beta1,j))\r\n",
        "                        v_hat_W[k] = v_W[k]/(math.pow(beta2,j))\r\n",
        "                        v_hat_b[k] = v_b[k]/(math.pow(beta2,j))\r\n",
        "                    for k in range(self.Number_of_Layers):\r\n",
        "                        self.W[k] = self.W[k] - (learning_rate*(beta1*m_hat_W[k] + (1-beta1)*delW[k]/(1-beta1)))/np.sqrt(v_hat_W[k] + epsilon)\r\n",
        "                        self.b[k] = self.b[k] - (learning_rate*(beta1*m_hat_b[k] + (1-beta1)*delb[k]/(1-beta1)))/np.sqrt(v_hat_b[k] + epsilon)\r\n",
        "                    delW, delb = self.initialize(self.Size_of_Input,self.Number_of_Layers,self.Number_of_Neuron_each_Layer)\r\n",
        "\r\n",
        "            accuracy = correct/X.shape[0]*100\r\n",
        "            v_error, v_accruracy = self.val_loss_and_accuracy(val_images, val_labels)\r\n",
        "            final_accuracy = accuracy\r\n",
        "            val_accuracy = v_accruracy\r\n",
        "            print(\"Train accuracy after {} epoch : \", final_accuracy)\r\n",
        "            print(\"Validation accuracy after {} epoch : \", val_accuracy)\r\n",
        "    \r\n",
        "    def val_loss_and_accuracy(self,val_data,val_labels):\r\n",
        "        val_correct = 0\r\n",
        "        val_error = 0\r\n",
        "        val_loss = []\r\n",
        "        val_accuracy = []\r\n",
        "        for i in range(val_data.shape[0]):\r\n",
        "            A,H,y_hat = self.forward_propagation(val_data[i])\r\n",
        "            val_error += -math.log(y_hat[int(val_labels[i])])\r\n",
        "            if np.argmax(y_hat) == int(val_labels[i]):\r\n",
        "                val_correct += 1\r\n",
        "        return val_error/val_data.shape[0], val_correct/val_data.shape[0]*100\r\n",
        "    \r\n",
        "    def test(self,test_data,test_labels):\r\n",
        "        y_hat = []\r\n",
        "        correct = 0\r\n",
        "        for i in range(test_data.shape[0]):\r\n",
        "            A,H,y_hat = self.forward_propagation(test_data[i])\r\n",
        "            if np.argmax(y_hat) == int(test_labels[i]):\r\n",
        "                correct+=1\r\n",
        "        print(\"Test Accuracy : \" , correct/test_data.shape[0]*100)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wM5hAmjCfRQq"
      },
      "source": [
        "obj = NeuralNet(Size_of_Input = 784, Number_of_Neuron_each_Layer = [32, 32,10], Number_of_Layers = 3, activation_function = 'ReLU', typeOfInit = 'xavier', L2reg_const=0)\r\n",
        "obj.optimize(normalized, train_labels, val_images,val_labels,'momentum', learning_rate = 0.0006, max_epochs = 10,batch_size = 64)\r\n",
        "obj.test(test_images, test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLMoWWyDEUgE"
      },
      "source": [
        "obj = NeuralNet(Size_of_Input = 784, Number_of_Neuron_each_Layer = [32, 32,10], Number_of_Layers = 3, activation_function = 'ReLU', typeOfInit = 'xavier', L2reg_const=0)\r\n",
        "obj.optimize(normalized, train_labels, val_images,val_labels, 'nag', learning_rate = 0.0005, max_epochs = 10,batch_size = 64)\r\n",
        "obj.test(test_images, test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HP5RoCMmEX8E"
      },
      "source": [
        "obj = NeuralNet(Size_of_Input = 784, Number_of_Neuron_each_Layer = [32,10], Number_of_Layers = 2, activation_function = 'tanh', typeOfInit = 'xavier', L2reg_const=0)\r\n",
        "obj.optimize(normalized, train_labels, val_images,val_labels,'nadam', learning_rate = 0.0005, max_epochs = 10,batch_size = 32)\r\n",
        "obj.test(test_images, test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}