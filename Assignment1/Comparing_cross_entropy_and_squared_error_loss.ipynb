{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Comparing cross_entropy and squared_error loss",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfXN9da_9oMv"
      },
      "source": [
        "### **Importing Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEhgNreByvTJ"
      },
      "source": [
        "import random\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from keras.datasets import fashion_mnist\r\n",
        "from sklearn.model_selection  import train_test_split\r\n",
        "import itertools\r\n",
        "import math\r\n",
        "!pip install wandb\r\n",
        "import wandb\r\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnOMsHbSJi5a"
      },
      "source": [
        "wandb.login()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxOmlOSN-Cnh"
      },
      "source": [
        "### **`Loading and processing data`**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_RoS2jwgbmG"
      },
      "source": [
        "(train_images, train_labels),(test_images, test_labels) = fashion_mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lUc5ltEQcM_"
      },
      "source": [
        "train_images, val_images, train_labels, val_labels  = train_test_split(train_images,train_labels,test_size=0.1,random_state = 42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fMVYaXPdYql"
      },
      "source": [
        "traind = train_images.reshape(train_images.shape[0],-1)\r\n",
        "mean = traind.mean(axis=0)\r\n",
        "centerd = traind -  mean\r\n",
        "max = centerd.max(axis=0)\r\n",
        "normalized = centerd/np.max(max)\r\n",
        "\r\n",
        "\r\n",
        "val_images = val_images.reshape(val_images.shape[0],-1)\r\n",
        "mean = val_images.mean(axis=0)\r\n",
        "centerd = val_images -  mean\r\n",
        "max = centerd.max(axis=0)\r\n",
        "val_images = centerd/np.max(max)\r\n",
        "\r\n",
        "\r\n",
        "test_images = test_images.reshape(test_images.shape[0],-1)\r\n",
        "mean = test_images.mean(axis=0)\r\n",
        "centerd = test_images -  mean\r\n",
        "max = centerd.max(axis=0)\r\n",
        "test_images = centerd/np.max(max)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ccGAXKwVIqW"
      },
      "source": [
        "### **Defining Neural Network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjyCexR0Iszw"
      },
      "source": [
        "class NeuralNet:\r\n",
        "    def __init__(self,Size_of_Input, Number_of_Neuron_each_Layer, Number_of_Layers, activation_function, typeOfInit, L2reg_const = 0):\r\n",
        "        self.activation_function = activation_function\r\n",
        "        self.Size_of_Input = Size_of_Input\r\n",
        "        self.Number_of_Layers = Number_of_Layers\r\n",
        "        self.Number_of_Neuron_each_Layer = Number_of_Neuron_each_Layer\r\n",
        "        self.L2reg_const = L2reg_const\r\n",
        "        self.W,self.b = self.initializer(typeOfInit)\r\n",
        "\r\n",
        "    \r\n",
        "    def initializer(self, init):        \r\n",
        "        W = []\r\n",
        "        b = []\r\n",
        "        if init == 'random':\r\n",
        "            W.append(np.random.randn(self.Number_of_Neuron_each_Layer[0], self.Size_of_Input))\r\n",
        "            for i in range(1,self.Number_of_Layers):\r\n",
        "                W.append(np.random.randn(self.Number_of_Neuron_each_Layer[i],self.Number_of_Neuron_each_Layer[i-1]))\r\n",
        "\r\n",
        "            for i in range(self.Number_of_Layers):\r\n",
        "                b.append(np.random.rand(self.Number_of_Neuron_each_Layer[i]))\r\n",
        "\r\n",
        "        elif (init == 'xavier'):\r\n",
        "            W.append(np.random.normal(0,math.sqrt(2/(self.Number_of_Neuron_each_Layer[0]+ self.Size_of_Input)), (self.Number_of_Neuron_each_Layer[0], self.Size_of_Input)))\r\n",
        "            for i in range(1,self.Number_of_Layers):\r\n",
        "                W.append(np.random.normal(0, math.sqrt(2/(self.Number_of_Neuron_each_Layer[i]+self.Number_of_Neuron_each_Layer[i-1])),(self.Number_of_Neuron_each_Layer[i],self.Number_of_Neuron_each_Layer[i-1])))\r\n",
        "\r\n",
        "            for i in range(self.Number_of_Layers):\r\n",
        "                b.append(np.random.rand(self.Number_of_Neuron_each_Layer[i]))\r\n",
        "        return W,b\r\n",
        "\r\n",
        "\r\n",
        "    def activation(self, Z):\r\n",
        "        if self.activation_function == 'ReLU':\r\n",
        "            return self.ReLU(Z)\r\n",
        "        elif self.activation_function == 'tanh':\r\n",
        "            return self.tanh(Z)\r\n",
        "        elif self.activation_function == 'sigmoid':\r\n",
        "            return self.sigmoid(Z)\r\n",
        "\r\n",
        "\r\n",
        "    def activation_derivative(self,Z):\r\n",
        "        if self.activation_function == 'ReLU':\r\n",
        "            return self.ReLU_derivative(Z)\r\n",
        "        elif self.activation_function == 'tanh':\r\n",
        "            return self.tanh_derivative(Z)\r\n",
        "        elif self.activation_function == 'sigmoid':\r\n",
        "            return self.sigmoid_derivative(Z)\r\n",
        "\r\n",
        "    def ReLU(self,Z):\r\n",
        "        return np.maximum(0,Z)\r\n",
        "\r\n",
        "    def ReLU_derivative(self,Z):\r\n",
        "        return [1 if x>0 else 0 for x in Z]\r\n",
        "\r\n",
        "    def tanh(self, Z):\r\n",
        "        return np.array([((np.exp(x) - np.exp(-x))/((np.exp(x) + np.exp(-x)))) for x in Z])\r\n",
        "                 \r\n",
        "    def tanh_derivative(self, Z):\r\n",
        "        return np.array(1 - self.tanh(Z)**2)\r\n",
        "                 \r\n",
        "    def sigmoid_derivative(self,Z):\r\n",
        "        return self.sigmoid(Z)*(1-self.sigmoid(Z))\r\n",
        "\r\n",
        "    def sigmoid(self,x):\r\n",
        "        return np.where(x>=0, 1/(1+np.exp(-x)), np.exp(x)/(1+np.exp(x)))\r\n",
        "    \r\n",
        "    def softmax_function(self,Z):\r\n",
        "            Z = Z - Z.max()\r\n",
        "            return (np.exp(Z)/np.sum(np.exp(Z),axis=0))\r\n",
        "\r\n",
        "    def forward_propagation(self,Input):\r\n",
        "        A = []\r\n",
        "        H = []\r\n",
        "        Input = np.array(Input)\r\n",
        "        A.append(self.W[0].dot(Input) + self.b[0])\r\n",
        "        for i in range(1, self.Number_of_Layers):\r\n",
        "            H.append(self.activation(A[-1]))\r\n",
        "            A.append(self.W[i].dot(H[-1]) + self.b[i])\r\n",
        "        y_hat = self.softmax_function(A[-1])\r\n",
        "        return A, H, y_hat\r\n",
        "\r\n",
        "    def backward_propagation(self, A, H, y_hat, y, Input, loss_type):\r\n",
        "        delA = []\r\n",
        "        delH = []\r\n",
        "        delW = []\r\n",
        "        delb = []\r\n",
        "        Input = np.array(Input)\r\n",
        "        H.insert(0,Input)\r\n",
        "        ey = np.zeros(self.Number_of_Neuron_each_Layer[-1])\r\n",
        "        ey[y] = 1\r\n",
        "        \r\n",
        "        # delA and delH have reverse indexing\r\n",
        "\r\n",
        "        if loss_type == 'squared_error':\r\n",
        "            delA.append(np.array((y_hat - ey)*(y_hat - y_hat**2)))\r\n",
        "        else:\r\n",
        "            delA.append(np.array(-(ey - y_hat)))\r\n",
        "        \r\n",
        "        for i in range(self.Number_of_Layers-1,-1,-1):\r\n",
        "            delW.insert(0,delA[-1].reshape(delA[-1].shape[0],1).dot(H[i].reshape(H[i].shape[0],1).T) + self.L2reg_const*self.W[i])\r\n",
        "            delb.insert(0,delA[-1])\r\n",
        "            delH.append(self.W[i].T.dot(delA[-1]))\r\n",
        "            if i-1>=0:\r\n",
        "                delA.append(np.multiply(delH[-1], self.activation_derivative(A[i-1])))\r\n",
        "        return delW,delb\r\n",
        "    \r\n",
        "    \r\n",
        "    def initialize(self, Size_of_Input,Number_of_Layers,Number_of_Neuron_each_Layer):\r\n",
        "        W, b = [], []\r\n",
        "        W.append(np.zeros((Number_of_Neuron_each_Layer[0], Size_of_Input)))\r\n",
        "        for i in range(1,Number_of_Layers):\r\n",
        "            W.append(np.zeros((Number_of_Neuron_each_Layer[i],Number_of_Neuron_each_Layer[i-1])))\r\n",
        "        for i in range(Number_of_Layers):\r\n",
        "            b.append(np.zeros(Number_of_Neuron_each_Layer[i]))            \r\n",
        "        return W, b\r\n",
        "\r\n",
        "    \r\n",
        "    def optimize(self, X, Y, val_images,val_labels,optimizer, learning_rate, max_epochs,batch_size, loss_type):\r\n",
        "        if optimizer == 'sgd':\r\n",
        "          self.stochastic_gradient_descent(X, Y, val_images,val_labels, learning_rate, max_epochs, loss_type)\r\n",
        "        elif optimizer == 'momentum':\r\n",
        "          self.momentum_gradient_descent(X, Y, val_images,val_labels, learning_rate, max_epochs,batch_size, loss_type)\r\n",
        "        elif optimizer == 'nag':\r\n",
        "          self.nesterov_accelerated_gradient_descent(X, Y, val_images,val_labels, learning_rate, max_epochs,batch_size, loss_type)\r\n",
        "        elif optimizer == 'rmsprop':\r\n",
        "          self.rmsprop(X, Y, val_images,val_labels, learning_rate, max_epochs,batch_size, loss_type)\r\n",
        "        elif optimizer == 'adam':\r\n",
        "          self.adam(X, Y, val_images,val_labels, learning_rate, max_epochs,batch_size, loss_type)\r\n",
        "        elif optimizer == 'nadam':\r\n",
        "          self.nadam(X, Y, val_images,val_labels, learning_rate, max_epochs,batch_size, loss_type)\r\n",
        "\r\n",
        "\r\n",
        "    def stochastic_gradient_descent(self,X, Y, val_images,val_labels, learning_rate, max_epochs, loss_type):\r\n",
        "        for j in range(max_epochs):\r\n",
        "            correct = 0\r\n",
        "            error = 0\r\n",
        "            delW, delb = self.initialize(self.Size_of_Input,self.Number_of_Layers,self.Number_of_Neuron_each_Layer)\r\n",
        "    \r\n",
        "            for i in range(X.shape[0]):\r\n",
        "                A,H,y_hat = self.forward_propagation(X[i])\r\n",
        "                s = [x.sum() for x in self.W]\r\n",
        "                    \r\n",
        "                if loss_type == \"squared_error\":\r\n",
        "                    error += -math.log(y_hat[Y[i]]) + self.L2reg_const/2*sum(s)\r\n",
        "                else:\r\n",
        "                    error += -math.log(y_hat[Y[i]]) + self.L2reg_const/2*sum(s)\r\n",
        "\r\n",
        "                delW,delb = self.backward_propagation(A,H,y_hat,Y[i],X[i], loss_type)\r\n",
        "\r\n",
        "                if(np.argmax(y_hat) == Y[i]):\r\n",
        "                    correct +=1\r\n",
        "                \r\n",
        "                for i in range(self.Number_of_Layers):\r\n",
        "                    self.W[i] = self.W[i] - learning_rate*delW[i]\r\n",
        "                    self.b[i] = self.b[i] - learning_rate*delb[i]\r\n",
        "\r\n",
        "            error /= X.shape[0]\r\n",
        "            accuracy = correct/X.shape[0]*100\r\n",
        "            v_error, v_accruracy = self.val_loss_and_accuracy(val_images, val_labels, loss_type)\r\n",
        "            wandb.log({'epoch' : j, 'train_loss' : error, 'train_accuracy' : accuracy,'valid_loss' : v_error,'valid_accuracy' : v_accruracy})\r\n",
        "\r\n",
        "\r\n",
        "    def momentum_gradient_descent(self,X, Y, val_images,val_labels, learning_rate, max_epochs,batch_size, loss_type, gamma = 0.6):\r\n",
        "        updateW, updateb = self.initialize(self.Size_of_Input,self.Number_of_Layers,self.Number_of_Neuron_each_Layer)\r\n",
        "\r\n",
        "        for j in range(max_epochs):\r\n",
        "            correct = 0\r\n",
        "            error = 0\r\n",
        "\r\n",
        "            delW, delb = self.initialize(self.Size_of_Input,self.Number_of_Layers,self.Number_of_Neuron_each_Layer)\r\n",
        "\r\n",
        "            for i in range(X.shape[0]):\r\n",
        "                A,H,y_hat = self.forward_propagation(X[i])\r\n",
        "\r\n",
        "                s = [x.sum() for x in self.W]\r\n",
        "                if loss_type == \"squared_error\":\r\n",
        "                    error += -math.log(y_hat[Y[i]]) + self.L2reg_const/2*sum(s)\r\n",
        "                else:\r\n",
        "                    error += -math.log(y_hat[Y[i]]) + self.L2reg_const/2*sum(s)\r\n",
        "\r\n",
        "                w,b = self.backward_propagation(A,H,y_hat,Y[i],X[i], loss_type)\r\n",
        "\r\n",
        "                for k in range(self.Number_of_Layers):\r\n",
        "                    delW[k] += w[k]\r\n",
        "                    delb[k] += b[k]\r\n",
        "\r\n",
        "                for k in range(self.Number_of_Layers):\r\n",
        "                    updateW[k] = gamma*updateW[k] + learning_rate*delW[k]   \r\n",
        "                    updateb[k] = gamma*updateb[k] + learning_rate*delb[k]\r\n",
        "                \r\n",
        "                if  (i%batch_size == 0 and i!=0) or i==X.shape[0]-1:\r\n",
        "                    delW, delb = self.initialize(self.Size_of_Input,self.Number_of_Layers,self.Number_of_Neuron_each_Layer)\r\n",
        "                    for k in range(self.Number_of_Layers):\r\n",
        "                        self.W[k] += -updateW[k]  \r\n",
        "                        self.b[k] += -updateb[k]\r\n",
        "\r\n",
        "                if(np.argmax(y_hat) == Y[i]):\r\n",
        "                    correct +=1\r\n",
        "\r\n",
        "                \r\n",
        "            error /= X.shape[0]\r\n",
        "            accuracy = correct/X.shape[0]*100\r\n",
        "            v_error, v_accruracy = self.val_loss_and_accuracy(val_images, val_labels, loss_type)\r\n",
        "            wandb.log({'epoch' : j, 'train_loss' : error, 'train_accuracy' : accuracy,'valid_loss' : v_error,'valid_accuracy' : v_accruracy})\r\n",
        "\r\n",
        "\r\n",
        "    def nesterov_accelerated_gradient_descent(self, X, Y, val_images,val_labels, learning_rate, max_epochs,batch_size,loss_type, gamma = 0.5):\r\n",
        "        updateW, updateb = self.initialize( self.Size_of_Input, self.Number_of_Layers,self.Number_of_Neuron_each_Layer)\r\n",
        "        lookaheadW, lookaheadb = self.initialize( self.Size_of_Input, self.Number_of_Layers,self.Number_of_Neuron_each_Layer)\r\n",
        "        thetaW, thetab = self.initialize( self.Size_of_Input, self.Number_of_Layers,self.Number_of_Neuron_each_Layer)\r\n",
        "\r\n",
        "        for j in range(max_epochs):\r\n",
        "            correct = 0\r\n",
        "            error = 0\r\n",
        "\r\n",
        "            delW, delb = self.initialize( self.Size_of_Input, self.Number_of_Layers,self.Number_of_Neuron_each_Layer)\r\n",
        "                \r\n",
        "            for k in range( self.Number_of_Layers):\r\n",
        "                thetaW[k] = self.W[k]\r\n",
        "                thetab[k] = self.b[k]\r\n",
        "\r\n",
        "            for k in range( self.Number_of_Layers):\r\n",
        "                lookaheadW[k] = thetaW[k] - gamma*updateW[k]    \r\n",
        "                lookaheadb[k] = thetab[k] - gamma*updateb[k]\r\n",
        "                self.W[k] = lookaheadW[k]\r\n",
        "                self.b[k] = lookaheadb[k]\r\n",
        "\r\n",
        "            \r\n",
        "            for i in range(X.shape[0]):\r\n",
        "                A,H,y_hat = self.forward_propagation(X[i])\r\n",
        "                \r\n",
        "                s = [x.sum() for x in self.W]\r\n",
        "                if loss_type == \"squared_error\":\r\n",
        "                    error += -math.log(y_hat[Y[i]]) + self.L2reg_const/2*sum(s)\r\n",
        "                else:\r\n",
        "                    error += -math.log(y_hat[Y[i]]) + self.L2reg_const/2*sum(s)\r\n",
        "\r\n",
        "                w,b = self.backward_propagation(A,H,y_hat,Y[i],X[i], loss_type)\r\n",
        "\r\n",
        "                for k in range( self.Number_of_Layers):\r\n",
        "                    delW[k] += w[k]\r\n",
        "                    delb[k] += b[k]\r\n",
        "\r\n",
        "                for k in range( self.Number_of_Layers):\r\n",
        "                    updateW[k] =  gamma*updateW[k] + learning_rate*delW[k]   \r\n",
        "                    updateb[k] = gamma*updateb[k] + learning_rate*delb[k]\r\n",
        "\r\n",
        "                \r\n",
        "                if  (i%batch_size == 0 and i!=0) or i==X.shape[0]-1:\r\n",
        "                    delW, delb = self.initialize(self.Size_of_Input,self.Number_of_Layers,self.Number_of_Neuron_each_Layer)\r\n",
        "                    for k in range(self.Number_of_Layers):\r\n",
        "                        self.W[k] += -updateW[k]  \r\n",
        "                        self.b[k] += -updateb[k]\r\n",
        "                \r\n",
        "                if(np.argmax(y_hat) == Y[i]):\r\n",
        "                    correct +=1\r\n",
        "            \r\n",
        "            error /= X.shape[0]\r\n",
        "            accuracy = correct/X.shape[0]*100\r\n",
        "            v_error, v_accruracy = self.val_loss_and_accuracy(val_images, val_labels, loss_type)\r\n",
        "            wandb.log({'epoch' : j, 'train_loss' : error, 'train_accuracy' : accuracy,'valid_loss' : v_error,'valid_accuracy' : v_accruracy})\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "    def rmsprop(self,X, Y, val_images,val_labels, learning_rate, max_epochs,batch_size,loss_type, beta = 0.89, epsilon = 1e-6):\r\n",
        "        v_W, v_b = self.initialize(self.Size_of_Input,self.Number_of_Layers,self.Number_of_Neuron_each_Layer)\r\n",
        "\r\n",
        "        for j in range(max_epochs):\r\n",
        "            correct = 0\r\n",
        "            error = 0\r\n",
        "\r\n",
        "            delW, delb = self.initialize(self.Size_of_Input,self.Number_of_Layers,self.Number_of_Neuron_each_Layer)\r\n",
        "    \r\n",
        "            for i in range(X.shape[0]):\r\n",
        "                A,H,y_hat = self.forward_propagation(X[i])\r\n",
        "                \r\n",
        "                s = [x.sum() for x in self.W]\r\n",
        "                if loss_type == \"squared_error\":\r\n",
        "                    error += -math.log(y_hat[Y[i]]) + self.L2reg_const/2*sum(s)\r\n",
        "                else:\r\n",
        "                    error += -math.log(y_hat[Y[i]]) + self.L2reg_const/2*sum(s)\r\n",
        "\r\n",
        "                w,b = self.backward_propagation(A,H,y_hat,Y[i],X[i], loss_type)\r\n",
        "\r\n",
        "                for k in range(self.Number_of_Layers):\r\n",
        "                    delW[k] += w[k]\r\n",
        "                    delb[k] += b[k]\r\n",
        "                \r\n",
        "                if(np.argmax(y_hat) == Y[i]):\r\n",
        "                    correct +=1\r\n",
        "\r\n",
        "                for k in range(self.Number_of_Layers):\r\n",
        "                    v_W[k] =  beta*v_W[k] + (1-beta)*delW[k]**2      \r\n",
        "                    v_b[k] = beta*v_b[k] + (1-beta)*delb[k]**2\r\n",
        "         \r\n",
        "                \r\n",
        "                if  (i%batch_size == 0 and i!=0) or i==X.shape[0]-1:\r\n",
        "                    for k in range(self.Number_of_Layers):\r\n",
        "                        v_W[k] =  beta*v_W[k] + (1-beta)*delW[k]**2      \r\n",
        "                        v_b[k] = beta*v_b[k] + (1-beta)*delb[k]**2\r\n",
        "                    for k in range(self.Number_of_Layers):\r\n",
        "                        self.W[k] = self.W[k] - (learning_rate*delW[k])/np.sqrt(v_W[k] + epsilon)\r\n",
        "                        self.b[k] = self.b[k] - (learning_rate*delb[k])/np.sqrt(v_b[k] + epsilon)\r\n",
        "                    delW, delb = self.initialize(self.Size_of_Input,self.Number_of_Layers,self.Number_of_Neuron_each_Layer) \r\n",
        "            \r\n",
        "            error /= X.shape[0]\r\n",
        "            accuracy = correct/X.shape[0]*100\r\n",
        "            v_error, v_accruracy = self.val_loss_and_accuracy(val_images, val_labels, loss_type)\r\n",
        "            wandb.log({'epoch' : j, 'train_loss' : error, 'train_accuracy' : accuracy,'valid_loss' : v_error,'valid_accuracy' : v_accruracy})\r\n",
        "\r\n",
        "\r\n",
        "    \r\n",
        "    def adam(self,X, Y, val_images,val_labels, learning_rate, max_epochs,batch_size, loss_type, beta1 = 0.89,beta2 = 0.989,epsilon = 1e-8):\r\n",
        "        m_W, m_b = self.initialize(self.Size_of_Input,self.Number_of_Layers,self.Number_of_Neuron_each_Layer)\r\n",
        "        m_hat_W, m_hat_b = self.initialize( self.Size_of_Input,self.Number_of_Layers,self.Number_of_Neuron_each_Layer)\r\n",
        "        v_W, v_b = self.initialize(self.Size_of_Input,self.Number_of_Layers,self.Number_of_Neuron_each_Layer)\r\n",
        "        v_hat_W, v_hat_b = self.initialize(self.Size_of_Input,self.Number_of_Layers,self.Number_of_Neuron_each_Layer)\r\n",
        "        \r\n",
        "        for j in range(max_epochs):\r\n",
        "            correct = 0\r\n",
        "            error = 0\r\n",
        "            delW, delb = self.initialize(self.Size_of_Input,self.Number_of_Layers,self.Number_of_Neuron_each_Layer)\r\n",
        "            \r\n",
        "            for i in range(X.shape[0]):\r\n",
        "                A,H,y_hat = self.forward_propagation(X[i])\r\n",
        "                \r\n",
        "                s = [x.sum() for x in self.W]\r\n",
        "                if loss_type == \"squared_error\":\r\n",
        "                    error += -math.log(y_hat[Y[i]]) + self.L2reg_const/2*sum(s)\r\n",
        "                else:\r\n",
        "                    error += -math.log(y_hat[Y[i]]) + self.L2reg_const/2*sum(s)\r\n",
        "\r\n",
        "                w,b = self.backward_propagation(A,H,y_hat,Y[i],X[i], loss_type)\r\n",
        "\r\n",
        "                for k in range(self.Number_of_Layers):\r\n",
        "                    delW[k] += w[k]\r\n",
        "                    delb[k] += b[k]\r\n",
        "                \r\n",
        "                if(np.argmax(y_hat) == Y[i]):\r\n",
        "                    correct +=1\r\n",
        "\r\n",
        "                if  (i%batch_size == 0 and i!=0) or i==X.shape[0]-1:\r\n",
        "                    for k in range(self.Number_of_Layers):\r\n",
        "                        v_W[k] =  beta2*v_W[k] + (1-beta2)*delW[k]*delW[k]\r\n",
        "                        v_b[k] = beta2*v_b[k] + (1-beta2)*delb[k]*delb[k]\r\n",
        "                        m_W[k] = beta1*m_W[k] + (1-beta1)*delW[k]\r\n",
        "                        m_b[k] = beta1*m_b[k] + (1-beta1)*delb[k]\r\n",
        "                        m_hat_W[k] = m_W[k]/(math.pow(beta1,j))\r\n",
        "                        m_hat_b[k] = m_b[k]/(math.pow(beta1,j))\r\n",
        "                        v_hat_W[k] = v_W[k]/(math.pow(beta2,j))\r\n",
        "                        v_hat_b[k] = v_b[k]/(math.pow(beta2,j))\r\n",
        "                    \r\n",
        "                    for k in range(self.Number_of_Layers):\r\n",
        "                        self.W[k] = self.W[k] - (learning_rate*m_hat_W[k])/np.sqrt(v_hat_W[k] + epsilon)\r\n",
        "                        self.b[k] = self.b[k] - (learning_rate*m_hat_b[k])/np.sqrt(v_hat_b[k] + epsilon)\r\n",
        "                    delW, delb = self.initialize(self.Size_of_Input,self.Number_of_Layers,self.Number_of_Neuron_each_Layer)\r\n",
        "                                \r\n",
        "            error /= X.shape[0]\r\n",
        "            accuracy = correct/X.shape[0]*100\r\n",
        "            v_error, v_accruracy = self.val_loss_and_accuracy(val_images, val_labels, loss_type)\r\n",
        "            wandb.log({'epoch' : j, 'train_loss' : error, 'train_accuracy' : accuracy,'valid_loss' : v_error,'valid_accuracy' : v_accruracy})\r\n",
        "    \r\n",
        "    def nadam(self, X, Y, val_images,val_labels, learning_rate, max_epochs,batch_size, loss_type, beta1 = 0.89,beta2 = 0.989,epsilon = 1e-8):\r\n",
        "        m_W, m_b = self.initialize(self.Size_of_Input,self.Number_of_Layers,self.Number_of_Neuron_each_Layer)\r\n",
        "        m_hat_W, m_hat_b = self.initialize(self.Size_of_Input,self.Number_of_Layers,self.Number_of_Neuron_each_Layer)\r\n",
        "        v_W, v_b = self.initialize(self.Size_of_Input,self.Number_of_Layers,self.Number_of_Neuron_each_Layer)\r\n",
        "        v_hat_W, v_hat_b = self.initialize(self.Size_of_Input,self.Number_of_Layers,self.Number_of_Neuron_each_Layer)\r\n",
        "        \r\n",
        "        for j in range(max_epochs):\r\n",
        "            correct = 0\r\n",
        "            error = 0\r\n",
        "\r\n",
        "            delW, delb = self.initialize(self.Size_of_Input,self.Number_of_Layers,self.Number_of_Neuron_each_Layer)\r\n",
        "        \r\n",
        "            for i in range(X.shape[0]):\r\n",
        "                A,H,y_hat = self.forward_propagation(X[i])\r\n",
        "                \r\n",
        "                s = [x.sum() for x in self.W]\r\n",
        "                if loss_type == \"squared_error\":\r\n",
        "                    error += -math.log(y_hat[Y[i]]) + self.L2reg_const/2*sum(s)\r\n",
        "                else:\r\n",
        "                    error += -math.log(y_hat[Y[i]]) + self.L2reg_const/2*sum(s)\r\n",
        "\r\n",
        "                w,b = self.backward_propagation(A,H,y_hat,Y[i],X[i], loss_type)\r\n",
        "\r\n",
        "                for k in range(self.Number_of_Layers):\r\n",
        "                    delW[k] += w[k]\r\n",
        "                    delb[k] += b[k]\r\n",
        "                \r\n",
        "                if(np.argmax(y_hat) == Y[i]):\r\n",
        "                    correct +=1\r\n",
        "\r\n",
        "                if  (i%batch_size == 0 and i!=0) or i==X.shape[0]-1:\r\n",
        "                    for k in range(self.Number_of_Layers):\r\n",
        "                        v_W[k] =  beta2*v_W[k] + (1-beta2)*delW[k]**2\r\n",
        "                        v_b[k] = beta2*v_b[k] + (1-beta2)*delb[k]**2\r\n",
        "                        m_W[k] = beta1*m_W[k] + (1-beta1)*delW[k]\r\n",
        "                        m_b[k] = beta1*m_b[k] + (1-beta1)*delb[k]\r\n",
        "                        m_hat_W[k] = m_W[k]/(math.pow(beta1,j))\r\n",
        "                        m_hat_b[k] = m_b[k]/(math.pow(beta1,j))\r\n",
        "                        v_hat_W[k] = v_W[k]/(math.pow(beta2,j))\r\n",
        "                        v_hat_b[k] = v_b[k]/(math.pow(beta2,j))\r\n",
        "                    for k in range(self.Number_of_Layers):\r\n",
        "                        self.W[k] = self.W[k] - (learning_rate*(beta1*m_hat_W[k] + (1-beta1)*delW[k]/(1-beta1)))/np.sqrt(v_hat_W[k] + epsilon)\r\n",
        "                        self.b[k] = self.b[k] - (learning_rate*(beta1*m_hat_b[k] + (1-beta1)*delb[k]/(1-beta1)))/np.sqrt(v_hat_b[k] + epsilon)\r\n",
        "                    delW, delb = self.initialize(self.Size_of_Input,self.Number_of_Layers,self.Number_of_Neuron_each_Layer)\r\n",
        "                    \r\n",
        "            error /= X.shape[0]\r\n",
        "            accuracy = correct/X.shape[0]*100\r\n",
        "            v_error, v_accruracy = self.val_loss_and_accuracy(val_images, val_labels, loss_type)\r\n",
        "            wandb.log({'epoch' : j, 'train_loss' : error, 'train_accuracy' : accuracy,'valid_loss' : v_error,'valid_accuracy' : v_accruracy})\r\n",
        "        \r\n",
        "    \r\n",
        "    def val_loss_and_accuracy(self,val_data,val_labels, loss_type):\r\n",
        "        val_correct = 0\r\n",
        "        val_error = 0\r\n",
        "        val_loss = []\r\n",
        "        val_accuracy = []\r\n",
        "        for i in range(val_data.shape[0]):\r\n",
        "            A,H,y_hat = self.forward_propagation(val_data[i])\r\n",
        "            s = [x.sum() for x in self.W]\r\n",
        "            if loss_type == \"squared_error\":\r\n",
        "                val_error += -math.log(y_hat[val_labels[i]]) + self.L2reg_const/2*sum(s)\r\n",
        "            else:\r\n",
        "                val_error += -math.log(y_hat[val_labels[i]]) + self.L2reg_const/2*sum(s)\r\n",
        "\r\n",
        "            if np.argmax(y_hat) == val_labels[i]:\r\n",
        "                val_correct += 1\r\n",
        "        return val_error/val_data.shape[0], val_correct/val_data.shape[0]*100\r\n",
        "\r\n",
        "\r\n",
        "    def test(self,test_data,test_labels):\r\n",
        "        correct = 0\r\n",
        "        y_hat = []\r\n",
        "        for i in range(test_data.shape[0]):\r\n",
        "            A,H,y = self.forward_propagation(test_data[i])\r\n",
        "            if np.argmax(y_hat) == test_labels[i]:\r\n",
        "                correct += 1\r\n",
        "            y_hat.append(y)\r\n",
        "        return np.argmax(np.array(y_hat),axis=1), correct/test_data.shape[0]*100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5HhRiSQU4Ct"
      },
      "source": [
        "### **Sweep (Hyperparameter tuning)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71BrOuAh1rv0"
      },
      "source": [
        "sweep_config = {\r\n",
        "    'method': 'grid',\r\n",
        "    'metric': {'goal': 'maximize', 'name': 'valid_accuracy'},\r\n",
        "    'parameters': {'activation_function': {'value': 'tanh'},\r\n",
        "                'batch_size': {'values': [32, 64]},\r\n",
        "                'epochs': {'value': 10},\r\n",
        "                'hidden_layer_size': {'value': 32},\r\n",
        "                'num_of_hidden_layers': { 'value' : 1 },\r\n",
        "                'learning_rate': {'values': [0.005, 0.0006]},\r\n",
        "                'optimizer': {'values': ['momentum', 'adam']},\r\n",
        "                'weight_decay': {'value': 0},\r\n",
        "                'weight_initialization': {'value': 'xavier'},\r\n",
        "                'loss_type': {'values': ['squared_error', 'cross_entropy']}}}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhpH7BwPP2NB"
      },
      "source": [
        "def train():\r\n",
        "    var1 = wandb.init()\r\n",
        "    var2 = var1.config\r\n",
        "    obj = NeuralNet(normalized.shape[1], list(itertools.chain(*[[var2.hidden_layer_size]*var2.num_of_hidden_layers, [10]])), var2.num_of_hidden_layers+1, var2.activation_function, var2.weight_initialization, var2.weight_decay)\r\n",
        "    obj.optimize(normalized, train_labels, val_images, val_labels, var2.optimizer, var2.learning_rate, var2.epochs, var2.batch_size, var2.loss_type)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5kCaKTjf9I5"
      },
      "source": [
        "sweep_id = wandb.sweep(sweep_config, project='CS6910 assignment1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmvCpkbegBus"
      },
      "source": [
        "wandb.agent(sweep_id, train)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}