{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural Network for Fashion MNIST.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfXN9da_9oMv"
      },
      "source": [
        "**Importing Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEhgNreByvTJ"
      },
      "source": [
        "import random\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from keras.datasets import fashion_mnist\r\n",
        "from sklearn.model_selection  import train_test_split\r\n",
        "import itertools\r\n",
        "import math\r\n",
        "!pip install wandb\r\n",
        "import wandb\r\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnOMsHbSJi5a"
      },
      "source": [
        "wandb.login()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxOmlOSN-Cnh"
      },
      "source": [
        "**`Loading and processing data`**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_RoS2jwgbmG"
      },
      "source": [
        "(train_images, train_labels),(test_images, test_labels) = fashion_mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5A9RIg4uhXWN"
      },
      "source": [
        "sample_labels = list(train_labels)\r\n",
        "names = [\"T-shirt/top\",\"Trouser/pants\",\"Pullover shirt\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\",\"Sneaker\",\"Bag\",\"Ankle boot\"]\r\n",
        "for i in range(10):\r\n",
        "    wandb.init(project=\"Assignment1\")\r\n",
        "    image_index = list.index(sample_labels, i)\r\n",
        "    print(image_index)\r\n",
        "    wandb.log({'label': i, 'image': [wandb.Image(train_images[image_index], caption='{}'.format(names[i]))]})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lUc5ltEQcM_"
      },
      "source": [
        "train_images, val_images, train_labels, val_labels  = train_test_split(train_images,train_labels,test_size=0.1,random_state = 42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fMVYaXPdYql"
      },
      "source": [
        "traind = train_images.reshape(train_images.shape[0],-1)\r\n",
        "mean = traind.mean(axis=0)\r\n",
        "centerd = traind -  mean\r\n",
        "max = centerd.max(axis=0)\r\n",
        "normalized = centerd/np.max(max)\r\n",
        "\r\n",
        "\r\n",
        "val_images = val_images.reshape(val_images.shape[0],-1)\r\n",
        "mean = val_images.mean(axis=0)\r\n",
        "centerd = val_images -  mean\r\n",
        "max = centerd.max(axis=0)\r\n",
        "val_images = centerd/np.max(max)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKv8hEvRD0r5"
      },
      "source": [
        "#parameters:\r\n",
        "Size_of_Input = normalized.shape[1]\r\n",
        "Number_of_Neuron_each_Layer = [32,10]\r\n",
        "Number_of_Layers = 2\r\n",
        "activation_function = 'sigmoid'\r\n",
        "weight_initializer = 'random'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObyLbY2ZAc6C"
      },
      "source": [
        "class NeuralNet:\r\n",
        "    def __init__(self,Size_of_Input, Number_of_Neuron_each_Layer, Number_of_Layers, activation_function, typeOfInit, L2reg_const = 0):\r\n",
        "        self.activation_function = activation_function\r\n",
        "        self.Size_of_Input = Size_of_Input\r\n",
        "        self.Number_of_Layers = Number_of_Layers\r\n",
        "        self.Number_of_Neuron_each_Layer = Number_of_Neuron_each_Layer\r\n",
        "        self.L2reg_const = L2reg_const\r\n",
        "        self.W,self.b = self.initializer(typeOfInit)\r\n",
        "\r\n",
        "    \r\n",
        "    def initializer(self, init):        \r\n",
        "        W = []\r\n",
        "        b = []\r\n",
        "        if init == 'random':\r\n",
        "            W.append(np.random.randn(self.Number_of_Neuron_each_Layer[0], self.Size_of_Input))\r\n",
        "            for i in range(1,self.Number_of_Layers):\r\n",
        "                W.append(np.random.randn(self.Number_of_Neuron_each_Layer[i],self.Number_of_Neuron_each_Layer[i-1]))\r\n",
        "\r\n",
        "            for i in range(self.Number_of_Layers):\r\n",
        "                b.append(np.random.rand(self.Number_of_Neuron_each_Layer[i]))\r\n",
        "        elif(init == 'xavier'):\r\n",
        "            W.append(np.random.normal(0,math.sqrt(2/(self.Number_of_Neuron_each_Layer[0]+ self.Size_of_Input)), (self.Number_of_Neuron_each_Layer[0], self.Size_of_Input)))\r\n",
        "            for i in range(1,self.Number_of_Layers):\r\n",
        "                W.append(np.random.normal(0, math.sqrt(2/(self.Number_of_Neuron_each_Layer[i]+self.Number_of_Neuron_each_Layer[i-1])),(self.Number_of_Neuron_each_Layer[i],self.Number_of_Neuron_each_Layer[i-1])))\r\n",
        "\r\n",
        "            for i in range(self.Number_of_Layers):\r\n",
        "                b.append(np.random.rand(self.Number_of_Neuron_each_Layer[i]))\r\n",
        "            np.random.normal(0.0, 1.0, (2,2))\r\n",
        "        return W,b\r\n",
        "\r\n",
        "\r\n",
        "    def activation(self, Z):\r\n",
        "        if self.activation_function == 'ReLU':\r\n",
        "            return self.ReLU(Z)\r\n",
        "        elif self.activation_function == 'tanh':\r\n",
        "            return self.tanh(Z)\r\n",
        "        elif self.activation_function == 'sigmoid':\r\n",
        "            return self.sigmoid(Z)\r\n",
        "\r\n",
        "\r\n",
        "    def activation_derivative(self,Z):\r\n",
        "        if self.activation_function == 'ReLU':\r\n",
        "            return self.ReLU_derivative(Z)\r\n",
        "        elif self.activation_function == 'tanh':\r\n",
        "            return self.tanh_derivative(Z)\r\n",
        "        elif self.activation_function == 'sigmoid':\r\n",
        "            return self.sigmoid_derivative(Z)\r\n",
        "\r\n",
        "    def ReLU(self,Z):\r\n",
        "        return np.maximum(0,Z)\r\n",
        "\r\n",
        "    def ReLU_derivative(self,Z):\r\n",
        "        return [1 if x>0 else 0 for x in Z]\r\n",
        "\r\n",
        "    def tanh(self, Z):\r\n",
        "        return np.array([((np.exp(x) - np.exp(-x))/((np.exp(x) + np.exp(-x)))) for x in Z])\r\n",
        "                 \r\n",
        "    def tanh_derivative(self, Z):\r\n",
        "        return np.array(1 - self.tanh(Z)**2)\r\n",
        "                 \r\n",
        "    def sigmoid_derivative(self,Z):\r\n",
        "        return self.sigmoid(Z)*(1-self.sigmoid(Z))\r\n",
        "\r\n",
        "    def sigmoid(self,x):\r\n",
        "        return np.where(x>=0, 1/(1+np.exp(-x)), np.exp(x)/(1+np.exp(x)))\r\n",
        "    \r\n",
        "    def softmax_function(self,Z):\r\n",
        "            Z = Z - Z.max()\r\n",
        "            return (np.exp(Z)/np.sum(np.exp(Z),axis=0))\r\n",
        "\r\n",
        "    def forward_propagation(self,Input):\r\n",
        "        A = []\r\n",
        "        H = []\r\n",
        "        Input = np.array(Input)\r\n",
        "        A.append(self.W[0].dot(Input) + self.b[0])\r\n",
        "        for i in range(1, self.Number_of_Layers):\r\n",
        "            H.append(self.activation(A[-1]))\r\n",
        "            A.append(self.W[i].dot(H[-1]) + self.b[i])\r\n",
        "        y_hat = self.softmax_function(A[-1])\r\n",
        "        return A, H, y_hat\r\n",
        "\r\n",
        "    def backward_propagation(self, A, H, y_hat, y, Input):\r\n",
        "        delA = []\r\n",
        "        delH = []\r\n",
        "        delW = []\r\n",
        "        delb = []\r\n",
        "        Input = np.array(Input)\r\n",
        "        H.insert(0,Input)\r\n",
        "        ey = np.zeros(self.Number_of_Neuron_each_Layer[-1])\r\n",
        "        ey[y] = 1\r\n",
        "        \r\n",
        "        # delA and delH have reverse indexing\r\n",
        "        delA.append(np.array(-(ey - y_hat)))\r\n",
        "        for i in range(self.Number_of_Layers-1,-1,-1):\r\n",
        "            delW.insert(0,delA[-1].reshape(delA[-1].shape[0],1).dot(H[i].reshape(H[i].shape[0],1).T) + self.L2reg_const*self.W[i])\r\n",
        "            delb.insert(0,delA[-1])\r\n",
        "            delH.append(self.W[i].T.dot(delA[-1]))\r\n",
        "            if i-1>=0:\r\n",
        "                delA.append(np.multiply(delH[-1], self.activation_derivative(A[i-1])))\r\n",
        "        return delW,delb\r\n",
        "    \r\n",
        "    \r\n",
        "    def initialize(self, Size_of_Input,Number_of_Layers,Number_of_Neuron_each_Layer):\r\n",
        "        W, b = [], []\r\n",
        "        W.append(np.zeros((Number_of_Neuron_each_Layer[0], Size_of_Input)))\r\n",
        "        for i in range(1,Number_of_Layers):\r\n",
        "            W.append(np.zeros((Number_of_Neuron_each_Layer[i],Number_of_Neuron_each_Layer[i-1])))\r\n",
        "        for i in range(Number_of_Layers):\r\n",
        "            b.append(np.zeros(Number_of_Neuron_each_Layer[i]))\r\n",
        "            \r\n",
        "        return W, b\r\n",
        "    \r\n",
        "    def optimize(self, X, Y, val_images,val_labels,optimizer, learning_rate, max_epochs,batch_size):\r\n",
        "        if optimizer == 'sgd':\r\n",
        "          self.stochastic_gradient_descent(X, Y, val_images,val_labels, learning_rate, max_epochs)\r\n",
        "        elif optimizer == 'momentum':\r\n",
        "          self.momentum_gradient_descent(X, Y, val_images,val_labels, learning_rate, max_epochs,batch_size)\r\n",
        "        elif optimizer == 'nag':\r\n",
        "          self.nesterov_accelerated_gradient_descent(X, Y, val_images,val_labels, learning_rate, max_epochs,batch_size)\r\n",
        "\r\n",
        "\r\n",
        "    def stochastic_gradient_descent(self,X, Y, val_images,val_labels, learning_rate, max_epochs):\r\n",
        "        for j in range(max_epochs):\r\n",
        "            correct = 0\r\n",
        "            error = 0\r\n",
        "            delW, delb = self.initialize(self.Size_of_Input,self.Number_of_Layers,self.Number_of_Neuron_each_Layer)\r\n",
        "    \r\n",
        "            for i in range(X.shape[0]):\r\n",
        "                A,H,y_hat = self.forward_propagation(X[i])\r\n",
        "                s = [x.sum() for x in self.W]\r\n",
        "                error += -math.log(y_hat[Y[i]]) + self.L2reg_const/2*sum(s)\r\n",
        "\r\n",
        "                delW,delb = self.backward_propagation(A,H,y_hat,Y[i],X[i])\r\n",
        "\r\n",
        "                if(np.argmax(y_hat) == Y[i]):\r\n",
        "                    correct +=1\r\n",
        "                \r\n",
        "                for i in range(self.Number_of_Layers):\r\n",
        "                    self.W[i] = self.W[i] - learning_rate*delW[i]\r\n",
        "                    self.b[i] = self.b[i] - learning_rate*delb[i]\r\n",
        "\r\n",
        "            error /= X.shape[0]\r\n",
        "            accuracy = correct/X.shape[0]*100\r\n",
        "            v_error, v_accruracy = self.val_loss_and_accuracy(val_images, val_labels)\r\n",
        "            return error, accuracy, v_error, v_accuracy\r\n",
        "\r\n",
        "    def momentum_gradient_descent(self,X, Y, val_images,val_labels, learning_rate, max_epochs,batch_size, gamma = 0.6):\r\n",
        "        updateW, updateb = self.initialize(self.Size_of_Input,self.Number_of_Layers,self.Number_of_Neuron_each_Layer)\r\n",
        "\r\n",
        "        for j in range(max_epochs):\r\n",
        "            correct = 0\r\n",
        "            error = 0\r\n",
        "\r\n",
        "            delW, delb = self.initialize(self.Size_of_Input,self.Number_of_Layers,self.Number_of_Neuron_each_Layer)\r\n",
        "\r\n",
        "            for i in range(X.shape[0]):\r\n",
        "                A,H,y_hat = self.forward_propagation(X[i])\r\n",
        "\r\n",
        "                s = [x.sum() for x in self.W]\r\n",
        "                error += -math.log(y_hat[Y[i]]) + self.L2reg_const/2*sum(s)\r\n",
        "                \r\n",
        "                w,b = self.backward_propagation(A,H,y_hat,Y[i],X[i])\r\n",
        "\r\n",
        "                for k in range(self.Number_of_Layers):\r\n",
        "                    delW[k] += w[k]\r\n",
        "                    delb[k] += b[k]\r\n",
        "\r\n",
        "                for k in range(self.Number_of_Layers):\r\n",
        "                    updateW[k] = gamma*updateW[k] + learning_rate*delW[k]   \r\n",
        "                    updateb[k] = gamma*updateb[k] + learning_rate*delb[k]\r\n",
        "\r\n",
        "\r\n",
        "                \r\n",
        "                if  (i%batch_size == 0 and i!=0) or i==X.shape[0]-1:\r\n",
        "                    delW, delb = self.initialize(self.Size_of_Input,self.Number_of_Layers,self.Number_of_Neuron_each_Layer)\r\n",
        "                    for k in range(self.Number_of_Layers):\r\n",
        "                        self.W[k] += -updateW[k]  \r\n",
        "                        self.b[k] += -updateb[k]\r\n",
        "\r\n",
        "                if(np.argmax(y_hat) == Y[i]):\r\n",
        "                    correct +=1\r\n",
        "\r\n",
        "                \r\n",
        "            error /= X.shape[0]\r\n",
        "            accuracy = correct/X.shape[0]*100\r\n",
        "            v_error, v_accruracy = self.val_loss_and_accuracy(val_images, val_labels)\r\n",
        "            return error, accuracy, v_error, v_accuracy\r\n",
        "\r\n",
        "    def nesterov_accelerated_gradient_descent(self, X, Y, val_images,val_labels, learning_rate, max_epochs,batch_size, gamma = 0.5):\r\n",
        "        updateW, updateb = self.initialize( self.Size_of_Input, self.Number_of_Layers,self.Number_of_Neuron_each_Layer)\r\n",
        "        lookaheadW, lookaheadb = self.initialize( self.Size_of_Input, self.Number_of_Layers,self.Number_of_Neuron_each_Layer)\r\n",
        "        thetaW, thetab = self.initialize( self.Size_of_Input, self.Number_of_Layers,self.Number_of_Neuron_each_Layer)\r\n",
        "\r\n",
        "        for j in range(max_epochs):\r\n",
        "            correct = 0\r\n",
        "            error = 0\r\n",
        "\r\n",
        "            delW, delb = self.initialize( self.Size_of_Input, self.Number_of_Layers,self.Number_of_Neuron_each_Layer)\r\n",
        "                \r\n",
        "            for k in range( self.Number_of_Layers):\r\n",
        "                thetaW[k] = self.W[k]\r\n",
        "                thetab[k] = self.b[k]\r\n",
        "\r\n",
        "            for k in range( self.Number_of_Layers):\r\n",
        "                lookaheadW[k] = thetaW[k] - gamma*updateW[k]    \r\n",
        "                lookaheadb[k] = thetab[k] - gamma*updateb[k]\r\n",
        "                self.W[k] = lookaheadW[k]\r\n",
        "                self.b[k] = lookaheadb[k]\r\n",
        "\r\n",
        "            \r\n",
        "            for i in range(X.shape[0]):\r\n",
        "                A,H,y_hat = self.forward_propagation(X[i])\r\n",
        "                \r\n",
        "                s = [x.sum() for x in self.W]\r\n",
        "                error += -math.log(y_hat[Y[i]]) + self.L2reg_const/2*sum(s)\r\n",
        "\r\n",
        "                w,b = self.backward_propagation(A,H,y_hat,Y[i],X[i])\r\n",
        "\r\n",
        "                for k in range( self.Number_of_Layers):\r\n",
        "                    delW[k] += w[k]\r\n",
        "                    delb[k] += b[k]\r\n",
        "\r\n",
        "                for k in range( self.Number_of_Layers):\r\n",
        "                    updateW[k] =  gamma*updateW[k] + learning_rate*delW[k]   \r\n",
        "                    updateb[k] = gamma*updateb[k] + learning_rate*delb[k]\r\n",
        "\r\n",
        "                \r\n",
        "                if  (i%batch_size == 0 and i!=0) or i==X.shape[0]-1:\r\n",
        "                    delW, delb = self.initialize(self.Size_of_Input,self.Number_of_Layers,self.Number_of_Neuron_each_Layer)\r\n",
        "                    for k in range(self.Number_of_Layers):\r\n",
        "                        self.W[k] += -updateW[k]  \r\n",
        "                        self.b[k] += -updateb[k]\r\n",
        "                \r\n",
        "                if(np.argmax(y_hat) == Y[i]):\r\n",
        "                    correct +=1\r\n",
        "            \r\n",
        "            error /= X.shape[0]\r\n",
        "            accuracy = correct/X.shape[0]*100\r\n",
        "            v_error, v_accruracy = self.val_loss_and_accuracy(val_images, val_labels)\r\n",
        "            return error, accuracy, v_error, v_accuracy\r\n",
        "          \r\n",
        "      def val_loss_and_accuracy(self,val_data,val_labels):\r\n",
        "        val_correct = 0\r\n",
        "        val_error = 0\r\n",
        "        val_loss = []\r\n",
        "        val_accuracy = []\r\n",
        "        for i in range(val_data.shape[0]):\r\n",
        "            A,H,y_hat = self.forward_propagation(val_data[i])\r\n",
        "            val_error += -math.log(y_hat[val_labels[i]])\r\n",
        "            if np.argmax(y_hat) == val_labels[i]:\r\n",
        "                val_correct += 1\r\n",
        "        return val_error/val_data.shape[0], val_correct/val_data.shape[0]*100"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
