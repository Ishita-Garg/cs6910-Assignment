{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural Network for Fashion MNIST.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfXN9da_9oMv"
      },
      "source": [
        "**Importing Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEhgNreByvTJ"
      },
      "source": [
        "import random\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from keras.datasets import fashion_mnist\r\n",
        "from sklearn.model_selection  import train_test_split\r\n",
        "import itertools\r\n",
        "import math\r\n",
        "!pip install wandb\r\n",
        "import wandb\r\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnOMsHbSJi5a"
      },
      "source": [
        "wandb.login()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxOmlOSN-Cnh"
      },
      "source": [
        "**`Loading and processing data`**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_RoS2jwgbmG"
      },
      "source": [
        "(train_images, train_labels),(test_images, test_labels) = fashion_mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5A9RIg4uhXWN"
      },
      "source": [
        "sample_labels = list(train_labels)\r\n",
        "names = [\"T-shirt/top\",\"Trouser/pants\",\"Pullover shirt\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\",\"Sneaker\",\"Bag\",\"Ankle boot\"]\r\n",
        "for i in range(10):\r\n",
        "    wandb.init(project=\"Assignment1\")\r\n",
        "    image_index = list.index(sample_labels, i)\r\n",
        "    print(image_index)\r\n",
        "    wandb.log({'label': i, 'image': [wandb.Image(train_images[image_index], caption='{}'.format(names[i]))]})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lUc5ltEQcM_"
      },
      "source": [
        "train_images, val_images, train_labels, val_labels  = train_test_split(train_images,train_labels,test_size=0.1,random_state = 42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fMVYaXPdYql"
      },
      "source": [
        "traind = train_images.reshape(train_images.shape[0],-1)\r\n",
        "mean = traind.mean(axis=0)\r\n",
        "centerd = traind -  mean\r\n",
        "max = centerd.max(axis=0)\r\n",
        "normalized = centerd/np.max(max)\r\n",
        "\r\n",
        "\r\n",
        "val_images = val_images.reshape(val_images.shape[0],-1)\r\n",
        "mean = val_images.mean(axis=0)\r\n",
        "centerd = val_images -  mean\r\n",
        "max = centerd.max(axis=0)\r\n",
        "val_images = centerd/np.max(max)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKv8hEvRD0r5"
      },
      "source": [
        "#parameters:\r\n",
        "Size_of_Input = normalized.shape[1]\r\n",
        "Number_of_Neuron_each_Layer = [32,10]\r\n",
        "Number_of_Layers = 2\r\n",
        "activation_function = 'sigmoid'\r\n",
        "weight_initializer = 'random'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObyLbY2ZAc6C"
      },
      "source": [
        "class NeuralNet:\r\n",
        "    def __init__(self,Size_of_Input, Number_of_Neuron_each_Layer, Number_of_Layers, activation_function, weight_initializer):\r\n",
        "        self.activation_function = activation_function\r\n",
        "        self.Size_of_Input = Size_of_Input\r\n",
        "        self.Number_of_Layers = Number_of_Layers\r\n",
        "        self.Number_of_Neuron_each_Layer = Number_of_Neuron_each_Layer\r\n",
        "        self.W,self.b = self.initializer(weight_initializer)\r\n",
        "\r\n",
        "    \r\n",
        "    def initializer(self, weight_initializer):        \r\n",
        "        W = []\r\n",
        "        b = []\r\n",
        "        if weight_initializer == 'random':\r\n",
        "            W.append(np.random.randn(self.Number_of_Neuron_each_Layer[0], self.Size_of_Input))\r\n",
        "            for i in range(1,self.Number_of_Layers):\r\n",
        "                W.append(np.random.randn(self.Number_of_Neuron_each_Layer[i],self.Number_of_Neuron_each_Layer[i-1]))\r\n",
        "\r\n",
        "            for i in range(self.Number_of_Layers):\r\n",
        "                b.append(np.random.rand(self.Number_of_Neuron_each_Layer[i]))\r\n",
        "        elif(weight_initializer == 'xavier'):\r\n",
        "            W.append(np.random.normal(0,math.sqrt(2/(self.Number_of_Neuron_each_Layer[0]+ self.Size_of_Input)), (self.Number_of_Neuron_each_Layer[0], self.Size_of_Input)))\r\n",
        "            for i in range(1,self.Number_of_Layers):\r\n",
        "                W.append(np.random.normal(0, math.sqrt(2/(self.Number_of_Neuron_each_Layer[i]+self.Number_of_Neuron_each_Layer[i-1])),(self.Number_of_Neuron_each_Layer[i],self.Number_of_Neuron_each_Layer[i-1])))\r\n",
        "\r\n",
        "            for i in range(self.Number_of_Layers):\r\n",
        "                b.append(np.random.rand(self.Number_of_Neuron_each_Layer[i]))\r\n",
        "            np.random.normal(0.0, 1.0, (2,2))\r\n",
        "        return W,b\r\n",
        "\r\n",
        "\r\n",
        "    def activation(self, Z):\r\n",
        "        if self.activation_function == 'ReLU':\r\n",
        "            return self.ReLU(Z)\r\n",
        "        elif self.activation_function == 'tanh':\r\n",
        "            return self.tanh(Z)\r\n",
        "        elif self.activation_function == 'sigmoid':\r\n",
        "            return self.sigmoid(Z)\r\n",
        "\r\n",
        "\r\n",
        "    def activation_derivative(self,Z):\r\n",
        "        if self.activation_function == 'ReLU':\r\n",
        "            return self.ReLU_derivative(Z)\r\n",
        "        elif self.activation_function == 'tanh':\r\n",
        "            return self.tanh_derivative(Z)\r\n",
        "        elif self.activation_function == 'sigmoid':\r\n",
        "            return self.sigmoid_derivative(Z)\r\n",
        "\r\n",
        "    def ReLU(self,Z):\r\n",
        "        return np.maximum(0,Z)\r\n",
        "\r\n",
        "    def ReLU_derivative(self,Z):\r\n",
        "        return [1 if x>0 else 0 for x in Z]\r\n",
        "\r\n",
        "    def tanh(self, Z):\r\n",
        "        return np.array([((np.exp(x) - np.exp(-x))/((np.exp(x) + np.exp(-x)))) for x in Z])\r\n",
        "                 \r\n",
        "    def tanh_derivative(self, Z):\r\n",
        "        return np.array(1 - self.tanh(Z)**2)\r\n",
        "                 \r\n",
        "    def sigmoid_derivative(self,Z):\r\n",
        "        return self.sigmoid(Z)*(1-self.sigmoid(Z))\r\n",
        "\r\n",
        "    def sigmoid(self,x):\r\n",
        "        return np.where(x>=0, 1/(1+np.exp(-x)), np.exp(x)/(1+np.exp(x)))\r\n",
        "    \r\n",
        "    def softmax_function(self,Z):\r\n",
        "        Z = Z - Z.max()\r\n",
        "        return (np.exp(Z)/np.sum(np.exp(Z),axis=0))\r\n",
        "\r\n",
        "    def forward_propogation(self,Input):\r\n",
        "        A = []\r\n",
        "        H = []\r\n",
        "        Input = np.array(Input.T)\r\n",
        "        A.append(self.W[0].dot(Input) + self.b[0].reshape(self.b[0].shape[0],1))\r\n",
        "        for i in range(1, self.Number_of_Layers):\r\n",
        "            H.append(self.activation(A[-1]))\r\n",
        "            A.append(self.W[i].dot(H[-1]) + self.b[i].reshape(self.b[i].shape[0],1))\r\n",
        "        y_hat = self.softmax_function(A[-1])\r\n",
        "        return np.argmax(y_hat,axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ha1x1CHsDrQ4"
      },
      "source": [
        "model = NeuralNet(Size_of_Input, Number_of_Neuron_each_Layer, Number_of_Layers, activation_function, weight_initializer)\r\n",
        "y_pred = model.forward_propogation(normalized)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}