{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ques2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyE0L0-n7X50"
      },
      "source": [
        "# **Importing required libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQYDsJecC7Ku"
      },
      "source": [
        "import numpy as np\n",
        "import csv\n",
        "import pandas as pd\n",
        "import random\n",
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras.layers import LSTM, SimpleRNN, GRU, Embedding, Dense, TimeDistributed, Concatenate, AdditiveAttention "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ccS87o9spZb"
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebd9RN8Az2lo"
      },
      "source": [
        "!pip install wandb\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnEZvPl28qoS"
      },
      "source": [
        "# **Downloading Data from Google Drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8h6FvpYBs0KP"
      },
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3jdVoAts2F9"
      },
      "source": [
        "id1 = '1D3TlW1FklEsr-rJUEF2JP_upwEUvbhKU'\n",
        "id2 = '1YKRR5D6ZRaFlG8rcrGJHZ0i9Zj3xPkh8'\n",
        "id3 = '1neNMvrSDHHi65LU27bXWixfFpYzlsvQd'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbpwAIzRtpBJ"
      },
      "source": [
        "downloaded1 = drive.CreateFile({'id':id1})\n",
        "downloaded2 = drive.CreateFile({'id':id2})\n",
        "downloaded3 = drive.CreateFile({'id':id3})\n",
        "downloaded1.GetContentFile('hi.translit.sampled.train.tsv')\n",
        "downloaded3.GetContentFile('hi.translit.sampled.dev.tsv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twh5WZtEECS7"
      },
      "source": [
        "tsv_file = open(\"hi.translit.sampled.train.tsv\")\n",
        "read_tsv = csv.reader(tsv_file, delimiter=\"\\t\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8Id1WLmxt7s"
      },
      "source": [
        "val_tsv_file = open(\"hi.translit.sampled.dev.tsv\")\n",
        "val_read_tsv = csv.reader(val_tsv_file, delimiter=\"\\t\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmfFvzeW866A"
      },
      "source": [
        "# **Processing training and validation data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4g5aurnxFkbW"
      },
      "source": [
        "# Training data\n",
        "devnagri = []\n",
        "english = []\n",
        "\n",
        "for i in read_tsv:   \n",
        "    devnagri.append(i[0])\n",
        "    english.append(i[1])\n",
        "\n",
        "devnagri = np.array(devnagri)\n",
        "english = np.array(english)\n",
        "\n",
        "# Validation data\n",
        "val_devnagri = []\n",
        "val_english = []\n",
        "\n",
        "for i in val_read_tsv:\n",
        "    val_devnagri.append(i[0])\n",
        "    val_english.append(i[1])\n",
        "\n",
        "val_devnagri = np.array(val_devnagri)\n",
        "val_english = np.array(val_english)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ug9QmqY2sTFu"
      },
      "source": [
        "for i in range(devnagri.shape[0]):\n",
        "    devnagri[i] = \"\\t\" + devnagri[i] + \"\\n\"\n",
        "    \n",
        "for i in range(val_devnagri.shape[0]):\n",
        "    val_devnagri[i] = \"\\t\" + val_devnagri[i] + \"\\n\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D030eDYkGAph"
      },
      "source": [
        "# Getting input and target language characters\n",
        "\n",
        "# Training set\n",
        "english_characters = set()\n",
        "devnagri_characters = set()\n",
        "\n",
        "for word in english:\n",
        "    for char in word:\n",
        "        if char not in english_characters:\n",
        "            english_characters.add(char)\n",
        "\n",
        "for word in devnagri:\n",
        "    for char in word:\n",
        "        if char not in devnagri_characters:\n",
        "            devnagri_characters.add(char)\n",
        "\n",
        "# Validation set\n",
        "v_english_characters = set()\n",
        "v_devnagri_characters = set()\n",
        "\n",
        "for word in val_english:\n",
        "    for char in word:\n",
        "        if char not in v_english_characters:\n",
        "            v_english_characters.add(char)\n",
        "\n",
        "for word in val_devnagri:\n",
        "    for char in word:\n",
        "        if char not in v_devnagri_characters:\n",
        "            v_devnagri_characters.add(char)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ei1sBXOgHfA9"
      },
      "source": [
        "english_characters = sorted(list(english_characters))\n",
        "devnagri_characters = sorted(list(devnagri_characters))\n",
        "\n",
        "num_encoder_tokens = len(english_characters)\n",
        "num_decoder_tokens = len(devnagri_characters)\n",
        "\n",
        "max_encoder_seq_length = max([len(txt) for txt in english])\n",
        "max_decoder_seq_length = max([len(txt) for txt in devnagri])\n",
        "\n",
        "# print(\"Number of samples:\", len(english))\n",
        "# print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
        "# print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
        "# print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
        "# print(\"Max sequence length for outputs:\", max_decoder_seq_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E__RRLttBOj_"
      },
      "source": [
        "# **Preparing Encoder and Decoder Inputs**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4O8I420Tv8r"
      },
      "source": [
        "# Preparing train encoder and decoder inputs\n",
        "\n",
        "input_token_index = dict([(char, i) for i, char in enumerate(english_characters)])\n",
        "target_token_index = dict([(char, i) for i, char in enumerate(devnagri_characters)])\n",
        "\n",
        "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n",
        "\n",
        "encoder_input_data = np.zeros((len(english), max_encoder_seq_length), dtype=\"float32\")\n",
        "decoder_input_data = np.zeros((len(english), max_decoder_seq_length), dtype=\"float32\")\n",
        "decoder_target_data = np.zeros((len(english), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\")\n",
        "\n",
        "for i, (english, devnagri) in enumerate(zip(english, devnagri)):\n",
        "    for t, char in enumerate(english):\n",
        "        encoder_input_data[i, t] = input_token_index[char]\n",
        "    \n",
        "    for t, char in enumerate(devnagri):\n",
        "        decoder_input_data[i, t] = target_token_index[char]\n",
        "        if t > 0:\n",
        "            # decoder_target_data will be ahead by one timestep and will not include the start character.\n",
        "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0T_CS5AZidG"
      },
      "source": [
        "# Preparing validation encoder and decoder inputs\n",
        "\n",
        "encoder_val_input_data = np.zeros((len(val_english), max_encoder_seq_length), dtype=\"float32\")\n",
        "decoder_val_input_data = np.zeros((len(val_english), max_decoder_seq_length), dtype=\"float32\")\n",
        "decoder_val_target_data = np.zeros((len(val_english), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\")\n",
        "\n",
        "for i, (e, d) in enumerate(zip(val_english, val_devnagri)):\n",
        "    for t, char in enumerate(e):\n",
        "        encoder_val_input_data[i, t] = input_token_index[char]\n",
        "  \n",
        "    for t, char in enumerate(d):\n",
        "        decoder_val_input_data[i, t] =  target_token_index[char]\n",
        "        if t > 0:\n",
        "            # decoder_target_data will be ahead by one timestep and will not include the start character.\n",
        "            decoder_val_target_data[i, t - 1, target_token_index[char]] = 1.0   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vz_BxeCmBGPJ"
      },
      "source": [
        "# **Defining Seq2Seq Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itjkckjgCoLW"
      },
      "source": [
        "def training(input_embedding_size, dp, cell_type, hidden_layer_size, num_encoder_layers, num_decoder_layers):\n",
        "    \n",
        "    # ENCODER\n",
        "\n",
        "    encoder_inputs = Input(shape=(max_encoder_seq_length,))\n",
        "    encoder_embedding = Embedding(num_encoder_tokens, input_embedding_size, trainable=True)(encoder_inputs)\n",
        "    \n",
        "    encoder_layers = []\n",
        "    encoder_states = []    \n",
        "    if cell_type == 'RNN':\n",
        "        encoder = SimpleRNN(hidden_layer_size, return_sequences=True, return_state=True, dropout = dp)\n",
        "        encoder_layers.append(encoder)\n",
        "        encoder_outputs, state_h = encoder(encoder_embedding)\n",
        "        encoder_states.append([state_h])\n",
        "        if num_encoder_layers > 1:\n",
        "            encoder = SimpleRNN(hidden_layer_size,return_sequences=True,return_state=True, dropout = dp) \n",
        "            encoder_layers.append(encoder)\n",
        "            encoder_outputs, state_h2 = encoder(encoder_outputs)\n",
        "            encoder_states.append([state_h2])\n",
        "        if num_encoder_layers > 2:\n",
        "            encoder = SimpleRNN(hidden_layer_size,return_sequences=True,return_state=True, dropout = dp) \n",
        "            encoder_layers.append(encoder)\n",
        "            encoder_outputs, state_h3 = encoder(encoder_outputs)\n",
        "            encoder_states.append([state_h3])\n",
        "        \n",
        "    elif cell_type == 'GRU':\n",
        "        encoder = GRU(hidden_layer_size, return_sequences=True, return_state=True, dropout = dp)\n",
        "        encoder_layers.append(encoder)\n",
        "        encoder_outputs, state_h = encoder(encoder_embedding)\n",
        "        encoder_states.append([state_h])\n",
        "        if num_encoder_layers > 1:\n",
        "            encoder = GRU(hidden_layer_size,return_sequences=True,return_state=True, dropout = dp) \n",
        "            encoder_layers.append(encoder)\n",
        "            encoder_outputs, state_h2 = encoder(encoder_outputs)\n",
        "            encoder_states.append([state_h2])\n",
        "        if num_encoder_layers > 2:\n",
        "            encoder = GRU(hidden_layer_size,return_sequences=True,return_state=True, dropout = dp) \n",
        "            encoder_layers.append(encoder)\n",
        "            encoder_outputs, state_h3 = encoder(encoder_outputs)\n",
        "            encoder_states.append([state_h3])\n",
        "       \n",
        "    else:\n",
        "        encoder = LSTM(hidden_layer_size, return_sequences=True, return_state=True, dropout = dp)\n",
        "        encoder_layers.append(encoder)\n",
        "        encoder_outputs, state_h, state_c = encoder(encoder_embedding)\n",
        "        encoder_states.append([state_h, state_c])\n",
        "        if num_encoder_layers > 1:\n",
        "            encoder = LSTM(hidden_layer_size,return_sequences=True,return_state=True, dropout = dp) \n",
        "            encoder_layers.append(encoder)\n",
        "            encoder_outputs, state_h2, state_c2 = encoder(encoder_outputs)\n",
        "            encoder_states.append([state_h2, state_c2])\n",
        "        if num_encoder_layers > 2:\n",
        "            encoder = LSTM(hidden_layer_size,return_sequences=True,return_state=True, dropout = dp) \n",
        "            encoder_layers.append(encoder)\n",
        "            encoder_outputs, state_h3, state_c3 = encoder(encoder_outputs)\n",
        "            encoder_states.append([state_h3, state_c3])\n",
        "\n",
        "    \n",
        "    # DECODER\n",
        "\n",
        "    decoder_inputs = Input(shape=(max_decoder_seq_length,))\n",
        "    decoder_embedding = Embedding(num_decoder_tokens, input_embedding_size, trainable=True)(decoder_inputs)\n",
        "\n",
        "    # We set up our decoder to return full output sequences, and to return internal states as well. \n",
        "    # We don't use the return states in the training model, but we will use them in inference.\n",
        "    \n",
        "    decoder_layers = []\n",
        "    if cell_type == 'RNN':\n",
        "        decoder_RNN = SimpleRNN(hidden_layer_size, return_sequences=True, return_state=True, dropout = dp)\n",
        "        decoder_layers.append(decoder_RNN)\n",
        "        decoder_outputs, _ = decoder_RNN(decoder_embedding, initial_state=encoder_states[0])\n",
        "        if num_decoder_layers > 1:\n",
        "            decoder_RNN = SimpleRNN(hidden_layer_size, return_sequences=True, return_state=True, dropout = dp)\n",
        "            decoder_layers.append(decoder_RNN)\n",
        "            decoder_outputs, _  = decoder_RNN(decoder_outputs, initial_state=encoder_states[1])\n",
        "        if num_decoder_layers > 2:\n",
        "            decoder_RNN = SimpleRNN(hidden_layer_size, return_sequences=True, return_state=True, dropout = dp)\n",
        "            decoder_layers.append(decoder_RNN)\n",
        "            decoder_outputs, _  = decoder_RNN(decoder_outputs, initial_state=encoder_states[2])\n",
        "        \n",
        "    elif cell_type == 'GRU':\n",
        "        decoder_GRU = GRU(hidden_layer_size, return_sequences=True, return_state=True, dropout = dp)\n",
        "        decoder_layers.append(decoder_GRU)\n",
        "        decoder_outputs, _ = decoder_GRU(decoder_embedding, initial_state=encoder_states[0])\n",
        "        if num_decoder_layers > 1:\n",
        "            decoder_GRU = GRU(hidden_layer_size, return_sequences=True, return_state=True, dropout = dp)\n",
        "            decoder_layers.append(decoder_GRU)\n",
        "            decoder_outputs, _  = decoder_GRU(decoder_outputs, initial_state=encoder_states[1])\n",
        "        if num_decoder_layers > 2:\n",
        "            decoder_GRU = GRU(hidden_layer_size, return_sequences=True, return_state=True, dropout = dp)\n",
        "            decoder_layers.append(decoder_GRU)\n",
        "            decoder_outputs, _  = decoder_GRU(decoder_outputs, initial_state=encoder_states[2])\n",
        "      \n",
        "    else:\n",
        "        decoder_lstm = LSTM(hidden_layer_size, return_sequences=True, return_state=True, dropout = dp)\n",
        "        decoder_layers.append(decoder_lstm)\n",
        "        decoder_outputs, _ , _ = decoder_lstm(decoder_embedding, initial_state=encoder_states[0])\n",
        "        if num_decoder_layers > 1:\n",
        "            decoder_lstm = LSTM(hidden_layer_size, return_sequences=True, return_state=True, dropout = dp)\n",
        "            decoder_layers.append(decoder_lstm)\n",
        "            decoder_outputs, _ , _  = decoder_lstm(decoder_outputs, initial_state=encoder_states[1])\n",
        "        if num_decoder_layers > 2:\n",
        "            decoder_lstm = LSTM(hidden_layer_size, return_sequences=True, return_state=True, dropout = dp)\n",
        "            decoder_layers.append(decoder_lstm)\n",
        "            decoder_outputs, _ , _  = decoder_lstm(decoder_outputs, initial_state=encoder_states[2])\n",
        "       \n",
        "    decoder_dense = TimeDistributed(Dense(num_decoder_tokens, activation=\"softmax\"))\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "    # MODEL\n",
        "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "    return model, encoder_layers, decoder_layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwCPBckEupTX"
      },
      "source": [
        "# **Inference model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvLRxv-cCoLa"
      },
      "source": [
        "def inferencing(model,num_encoder_layers,num_decoder_layers,encoder_layers,decoder_layers,cell_type, hidden_layer_size):\n",
        "    \n",
        "    # ENCODER MODEL RECONSTRUCTION \n",
        "    encoder_inputs = model.input[0]  # input_1\n",
        "    encoder_states = []\n",
        "    enc_emb = model.layers[2]     # embedding 1\n",
        "    encoder_outputs = enc_emb(encoder_inputs)\n",
        "\n",
        "    if cell_type == 'RNN' or cell_type ==\"GRU\":\n",
        "        for i in range(num_encoder_layers):\n",
        "            encoder_outputs, state_h_enc = encoder_layers[i](encoder_outputs)\n",
        "            encoder_states += [state_h_enc] \n",
        "    else:\n",
        "        for i in range(num_encoder_layers):\n",
        "            encoder_outputs, state_h_enc, state_c_enc = encoder_layers[i](encoder_outputs)\n",
        "            encoder_states += [state_h_enc, state_c_enc]   \n",
        "\n",
        "    encoder_model = Model(encoder_inputs, encoder_states + [encoder_outputs])\n",
        "\n",
        "\n",
        "    # DECODER MODEL RECONSTRUCTION\n",
        "    input_names = [[\"input_100\",\"input_101\"],[\"input_102\",\"input_103\"],[\"input_104\",\"input_105\"],\"input_106\"]\n",
        "\n",
        "    decoder_inputs = model.input[1]       # input_2\n",
        "    decoder_embedding = model.layers[3]   # embedding 2\n",
        "    decoder_outputs = decoder_embedding(decoder_inputs)\n",
        "    decoder_states = []\n",
        "    decoder_states_inputs = []\n",
        "    \n",
        "    if cell_type == 'RNN' or cell_type ==\"GRU\":\n",
        "        for i in range(num_decoder_layers):\n",
        "            decoder_states_inputs += [Input(shape=(hidden_layer_size,), name=input_names[i][0])]\n",
        "        for i in range(num_decoder_layers):\n",
        "            decoder_outputs, state_h_dec = decoder_layers[i](decoder_outputs, initial_state=decoder_states_inputs[i])\n",
        "            decoder_states += [state_h_dec]\n",
        "    else:\n",
        "        for i in range(num_decoder_layers):\n",
        "            decoder_states_inputs += [Input(shape=(hidden_layer_size,), name=input_names[i][0]), Input(shape=(hidden_layer_size,), name=input_names[i][1])]\n",
        "        j = 0\n",
        "        for i in range(num_decoder_layers):\n",
        "            decoder_outputs, state_h_dec, state_c_dec = decoder_layers[i](decoder_outputs, initial_state=decoder_states_inputs[i+j:i+j+2])\n",
        "            decoder_states += [state_h_dec , state_c_dec]\n",
        "            j += 1\n",
        "\n",
        "    decoder_dense = model.layers[4+2*num_encoder_layers]\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "    decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
        "\n",
        "    return encoder_model, decoder_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKU9UeA7q75x"
      },
      "source": [
        "def decode_sequence(input_seq,encoder_model,decoder_model):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "    states_value = states_value[:-1]\n",
        "    target_seq = np.zeros((1, 1)) \n",
        "    target_seq[0, 0] = target_token_index[\"\\t\"]\n",
        "    stop_condition = False\n",
        "    decoded_sentence = \"\"\n",
        "    while not stop_condition:\n",
        "        dec_ip = [target_seq]+states_value\n",
        "        output_tokens = decoder_model.predict(dec_ip)\n",
        "        sampled_token_index = np.argmax(output_tokens[0][0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n",
        "            stop_condition = True\n",
        "\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "        states_value = output_tokens[1:]\n",
        "        \n",
        "    return decoded_sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rf15d9ABWI0"
      },
      "source": [
        "# **Fitting the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3tft1XVCoLY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb414abe-fb07-49ad-ccc2-4ff6e033c3e7"
      },
      "source": [
        "# batch_size = 128        \n",
        "# epochs = 7             \n",
        "# input_embedding_size = 512\n",
        "# hidden_layer_size = 256\n",
        "# num_layers = 3\n",
        "# num_encoder_layers = num_layers\n",
        "# num_decoder_layers = num_layers\n",
        "# dropout = 0.2\n",
        "# cell_type = 'LSTM'\n",
        "\n",
        "# # TRAIN\n",
        "# model, encoder_layers, decoder_layers = training(input_embedding_size, dropout, cell_type, hidden_layer_size, num_encoder_layers, num_decoder_layers)\n",
        "\n",
        "# # COMPILE\n",
        "# model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "# # FIT\n",
        "# model.fit(\n",
        "#     [encoder_input_data, decoder_input_data],\n",
        "#     decoder_target_data,\n",
        "#     batch_size=batch_size,\n",
        "#     epochs=epochs,\n",
        "#     shuffle = True,\n",
        "#     # validation_data= ([encoder_val_input_data, decoder_val_input_data], decoder_val_target_data)\n",
        "# )\n",
        "\n",
        "# encoder_model, decoder_model = inferencing(model, num_encoder_layers, num_decoder_layers, encoder_layers, decoder_layers, cell_type, hidden_layer_size)\n",
        "# correct = 0\n",
        "# n = val_devnagri.shape[0]\n",
        "# for i in range(n):\n",
        "#     input = encoder_val_input_data[i:i+1]\n",
        "#     output = decode_sequence(input,encoder_model, decoder_model)\n",
        "#     if output.strip() == val_devnagri[i].strip():\n",
        "#         correct += 1\n",
        "# print(\"Validation accuracy : \", correct*100/n)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9mXbfsmyk5Z"
      },
      "source": [
        "# **Hyperparameter Tuning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRazj-4M8VYI"
      },
      "source": [
        "sweep_config = {\n",
        "    'method': 'bayes',\n",
        "    'metric': {'goal': 'maximize', 'name': 'val_accuracy'},\n",
        "    'parameters': {'input_embedding_size': {'values': [128, 256, 512]},\n",
        "                   'hidden_layer_size': {'values': [128, 256, 512]},\n",
        "                   'cell_type': {'values': ['LSTM', 'RNN', 'GRU']},\n",
        "                   'num_layers': {'values': [1,2,3]},\n",
        "                   'batch_size': {'values': [128,256,512]},\n",
        "                   'dropout': {'values': [0.1, 0.2, 0.3, 0.4]}\n",
        "                }}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2t_HvCk0vBM"
      },
      "source": [
        "def train():\n",
        "    var1 = wandb.init()\n",
        "    var2 = var1.config\n",
        "    epochs = 10\n",
        "\n",
        "    model, encoder_layers, decoder_layers = training(var2.input_embedding_size, var2.dropout, var2.cell_type , var2.hidden_layer_size, var2.num_layers, var2.num_layers)\n",
        "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "    model.fit(\n",
        "        [encoder_input_data, decoder_input_data],\n",
        "        decoder_target_data,\n",
        "        batch_size=var2.batch_size,\n",
        "        epochs=epochs,\n",
        "        callbacks=[WandbCallback()]\n",
        "    )\n",
        "\n",
        "    encoder_model, decoder_model = inferencing(model,var2.num_layers, var2.num_layers,encoder_layers,decoder_layers,var2.cell_type,var2.hidden_layer_size)\n",
        "    correct = 0\n",
        "    n = val_devnagri.shape[0]\n",
        "    for i in range(n):\n",
        "        input = encoder_val_input_data[i:i+1]\n",
        "        output = decode_sequence(input,encoder_model, decoder_model)\n",
        "        if output.strip() == val_devnagri[i].strip():\n",
        "            correct += 1\n",
        "    wandb.log({'val_accuracy' : correct*100/n})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMR1I7lL049l"
      },
      "source": [
        "sweep_id = wandb.sweep(sweep_config, project=\"CS6910 Assignment 3\")\n",
        "wandb.agent(sweep_id, train, count=28)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}