{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ques5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyE0L0-n7X50"
      },
      "source": [
        "# **Importing required libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQYDsJecC7Ku"
      },
      "source": [
        "import numpy as np\n",
        "import csv\n",
        "import pandas as pd\n",
        "import random\n",
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras.layers import LSTM, SimpleRNN, GRU, Embedding, Dense, TimeDistributed, Concatenate, AdditiveAttention "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ccS87o9spZb"
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebd9RN8Az2lo"
      },
      "source": [
        "!pip install wandb\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.font_manager import FontProperties\n",
        "from IPython.display import HTML as html_print\n",
        "import time\n",
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnEZvPl28qoS"
      },
      "source": [
        "# **Downloading Data from Google Drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8h6FvpYBs0KP"
      },
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3jdVoAts2F9"
      },
      "source": [
        "id1 = '1D3TlW1FklEsr-rJUEF2JP_upwEUvbhKU'\n",
        "id2 = '1YKRR5D6ZRaFlG8rcrGJHZ0i9Zj3xPkh8'\n",
        "id3 = '1neNMvrSDHHi65LU27bXWixfFpYzlsvQd'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbpwAIzRtpBJ"
      },
      "source": [
        "downloaded1 = drive.CreateFile({'id':id1})\n",
        "downloaded2 = drive.CreateFile({'id':id2})\n",
        "downloaded3 = drive.CreateFile({'id':id3})\n",
        "downloaded1.GetContentFile('hi.translit.sampled.train.tsv')\n",
        "downloaded2.GetContentFile('hi.translit.sampled.test.tsv')\n",
        "downloaded3.GetContentFile('hi.translit.sampled.dev.tsv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twh5WZtEECS7"
      },
      "source": [
        "tsv_file = open(\"hi.translit.sampled.train.tsv\")\n",
        "read_tsv = csv.reader(tsv_file, delimiter=\"\\t\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8Id1WLmxt7s"
      },
      "source": [
        "val_tsv_file = open(\"hi.translit.sampled.dev.tsv\")\n",
        "val_read_tsv = csv.reader(val_tsv_file, delimiter=\"\\t\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s15tGAPM8v1S"
      },
      "source": [
        "test_tsv_file = open(\"hi.translit.sampled.test.tsv\")\n",
        "test_read_tsv = csv.reader(test_tsv_file, delimiter=\"\\t\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!wget \"https://www.fontmirror.com/app_public/files/t/1/2020/04/MANGAL.TTF\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmfFvzeW866A"
      },
      "source": [
        "# **Processing training, validation and test data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4g5aurnxFkbW"
      },
      "source": [
        "# Training data\n",
        "devnagri = []\n",
        "english = []\n",
        "\n",
        "for i in read_tsv:   \n",
        "    devnagri.append(i[0])\n",
        "    english.append(i[1])\n",
        "\n",
        "devnagri = np.array(devnagri)\n",
        "english = np.array(english)\n",
        "\n",
        "# Validation data\n",
        "val_devnagri = []\n",
        "val_english = []\n",
        "\n",
        "for i in val_read_tsv:\n",
        "    val_devnagri.append(i[0])\n",
        "    val_english.append(i[1])\n",
        "\n",
        "val_devnagri = np.array(val_devnagri)\n",
        "val_english = np.array(val_english)\n",
        "\n",
        "# Test data\n",
        "test_devnagri = []\n",
        "test_english = []\n",
        "\n",
        "for i in test_read_tsv:\n",
        "    test_devnagri.append(i[0])\n",
        "    test_english.append(i[1])\n",
        "\n",
        "test_devnagri = np.array(test_devnagri)\n",
        "test_english = np.array(test_english)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ug9QmqY2sTFu"
      },
      "source": [
        "for i in range(devnagri.shape[0]):\n",
        "    devnagri[i] = \"\\t\" + devnagri[i] + \"\\n\"\n",
        "    \n",
        "for i in range(val_devnagri.shape[0]):\n",
        "    val_devnagri[i] = \"\\t\" + val_devnagri[i] + \"\\n\"\n",
        "\n",
        "for i in range(test_devnagri.shape[0]):\n",
        "    test_devnagri[i] = \"\\t\" + test_devnagri[i] + \"\\n\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D030eDYkGAph"
      },
      "source": [
        "# Getting input and target language characters\n",
        "\n",
        "# Training set\n",
        "english_characters = set()\n",
        "devnagri_characters = set()\n",
        "\n",
        "for word in english:\n",
        "    for char in word:\n",
        "        if char not in english_characters:\n",
        "            english_characters.add(char)\n",
        "\n",
        "for word in devnagri:\n",
        "    for char in word:\n",
        "        if char not in devnagri_characters:\n",
        "            devnagri_characters.add(char)\n",
        "\n",
        "# Validation set\n",
        "v_english_characters = set()\n",
        "v_devnagri_characters = set()\n",
        "\n",
        "for word in val_english:\n",
        "    for char in word:\n",
        "        if char not in v_english_characters:\n",
        "            v_english_characters.add(char)\n",
        "\n",
        "for word in val_devnagri:\n",
        "    for char in word:\n",
        "        if char not in v_devnagri_characters:\n",
        "            v_devnagri_characters.add(char)\n",
        "\n",
        "# Test set\n",
        "t_english_characters = set()\n",
        "t_devnagri_characters = set()\n",
        "\n",
        "for word in test_english:\n",
        "    for char in word:\n",
        "        if char not in t_english_characters:\n",
        "            t_english_characters.add(char)\n",
        "\n",
        "for word in test_devnagri:\n",
        "    for char in word:\n",
        "        if char not in t_devnagri_characters:\n",
        "            t_devnagri_characters.add(char)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ei1sBXOgHfA9"
      },
      "source": [
        "english_characters = sorted(list(english_characters))\n",
        "devnagri_characters = sorted(list(devnagri_characters))\n",
        "\n",
        "num_encoder_tokens = len(english_characters)\n",
        "num_decoder_tokens = len(devnagri_characters)\n",
        "\n",
        "max_encoder_seq_length = max([len(txt) for txt in english])\n",
        "max_decoder_seq_length = max([len(txt) for txt in devnagri])\n",
        "\n",
        "# print(\"Number of samples:\", len(english))\n",
        "# print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
        "# print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
        "# print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
        "# print(\"Max sequence length for outputs:\", max_decoder_seq_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E__RRLttBOj_"
      },
      "source": [
        "# **Preparing Encoder and Decoder Inputs**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4O8I420Tv8r"
      },
      "source": [
        "# Preparing train encoder and decoder inputs\n",
        "\n",
        "input_token_index = dict([(char, i) for i, char in enumerate(english_characters)])\n",
        "target_token_index = dict([(char, i) for i, char in enumerate(devnagri_characters)])\n",
        "\n",
        "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n",
        "\n",
        "encoder_input_data = np.zeros((len(english), max_encoder_seq_length), dtype=\"float32\")\n",
        "decoder_input_data = np.zeros((len(english), max_decoder_seq_length), dtype=\"float32\")\n",
        "decoder_target_data = np.zeros((len(english), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\")\n",
        "\n",
        "for i, (english, devnagri) in enumerate(zip(english, devnagri)):\n",
        "    for t, char in enumerate(english):\n",
        "        encoder_input_data[i, t] = input_token_index[char]\n",
        "    \n",
        "    for t, char in enumerate(devnagri):\n",
        "        decoder_input_data[i, t] = target_token_index[char]\n",
        "        if t > 0:\n",
        "            # decoder_target_data will be ahead by one timestep and will not include the start character.\n",
        "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0T_CS5AZidG"
      },
      "source": [
        "# Preparing validation encoder and decoder inputs\n",
        "\n",
        "encoder_val_input_data = np.zeros((len(val_english), max_encoder_seq_length), dtype=\"float32\")\n",
        "decoder_val_input_data = np.zeros((len(val_english), max_decoder_seq_length), dtype=\"float32\")\n",
        "decoder_val_target_data = np.zeros((len(val_english), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\")\n",
        "\n",
        "for i, (e, d) in enumerate(zip(val_english, val_devnagri)):\n",
        "    for t, char in enumerate(e):\n",
        "        encoder_val_input_data[i, t] = input_token_index[char]\n",
        "  \n",
        "    for t, char in enumerate(d):\n",
        "        decoder_val_input_data[i, t] =  target_token_index[char]\n",
        "        if t > 0:\n",
        "            # decoder_target_data will be ahead by one timestep and will not include the start character.\n",
        "            decoder_val_target_data[i, t - 1, target_token_index[char]] = 1.0   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wssPWN9qBUwe"
      },
      "source": [
        "# Preparing test encoder and decoder inputs\n",
        "\n",
        "encoder_test_input_data = np.zeros((len(test_english), max_encoder_seq_length), dtype=\"float32\")\n",
        "decoder_test_input_data = np.zeros((len(test_english), max_decoder_seq_length), dtype=\"float32\")\n",
        "decoder_test_target_data = np.zeros((len(test_english), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\")\n",
        "\n",
        "for i, (e, d) in enumerate(zip(test_english, test_devnagri)):\n",
        "    for t, char in enumerate(e):\n",
        "        encoder_test_input_data[i, t] = input_token_index[char]\n",
        "    \n",
        "    for t, char in enumerate(d):\n",
        "        decoder_test_input_data[i, t] =  target_token_index[char]\n",
        "        if t > 0:\n",
        "            # decoder_target_data will be ahead by one timestep and will not include the start character.\n",
        "            decoder_test_target_data[i, t - 1, target_token_index[char]] = 1.0   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vz_BxeCmBGPJ"
      },
      "source": [
        "# **Defining Seq2Seq Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itjkckjgCoLW"
      },
      "source": [
        "def training(input_embedding_size, dp, cell_type, hidden_layer_size, num_encoder_layers, num_decoder_layers):\n",
        "    \n",
        "    # ENCODER\n",
        "\n",
        "    encoder_inputs = Input(shape=(max_encoder_seq_length,))\n",
        "    encoder_embedding = Embedding(num_encoder_tokens, input_embedding_size, trainable=True)(encoder_inputs)\n",
        "    \n",
        "    encoder_layers = []\n",
        "    encoder_states = []    \n",
        "    if cell_type == 'RNN':\n",
        "        encoder = SimpleRNN(hidden_layer_size, return_sequences=True, return_state=True, dropout = dp)\n",
        "        encoder_layers.append(encoder)\n",
        "        encoder_outputs, state_h = encoder(encoder_embedding)\n",
        "        encoder_states.append([state_h])\n",
        "        if num_encoder_layers > 1:\n",
        "            encoder = SimpleRNN(hidden_layer_size,return_sequences=True,return_state=True, dropout = dp) \n",
        "            encoder_layers.append(encoder)\n",
        "            encoder_outputs, state_h2 = encoder(encoder_outputs)\n",
        "            encoder_states.append([state_h2])\n",
        "        if num_encoder_layers > 2:\n",
        "            encoder = SimpleRNN(hidden_layer_size,return_sequences=True,return_state=True, dropout = dp) \n",
        "            encoder_layers.append(encoder)\n",
        "            encoder_outputs, state_h3 = encoder(encoder_outputs)\n",
        "            encoder_states.append([state_h3])\n",
        "        \n",
        "    elif cell_type == 'GRU':\n",
        "        encoder = GRU(hidden_layer_size, return_sequences=True, return_state=True, dropout = dp)\n",
        "        encoder_layers.append(encoder)\n",
        "        encoder_outputs, state_h = encoder(encoder_embedding)\n",
        "        encoder_states.append([state_h])\n",
        "        if num_encoder_layers > 1:\n",
        "            encoder = GRU(hidden_layer_size,return_sequences=True,return_state=True, dropout = dp) \n",
        "            encoder_layers.append(encoder)\n",
        "            encoder_outputs, state_h2 = encoder(encoder_outputs)\n",
        "            encoder_states.append([state_h2])\n",
        "        if num_encoder_layers > 2:\n",
        "            encoder = GRU(hidden_layer_size,return_sequences=True,return_state=True, dropout = dp) \n",
        "            encoder_layers.append(encoder)\n",
        "            encoder_outputs, state_h3 = encoder(encoder_outputs)\n",
        "            encoder_states.append([state_h3])\n",
        "       \n",
        "    else:\n",
        "        encoder = LSTM(hidden_layer_size, return_sequences=True, return_state=True, dropout = dp)\n",
        "        encoder_layers.append(encoder)\n",
        "        encoder_outputs, state_h, state_c = encoder(encoder_embedding)\n",
        "        encoder_states.append([state_h, state_c])\n",
        "        if num_encoder_layers > 1:\n",
        "            encoder = LSTM(hidden_layer_size,return_sequences=True,return_state=True, dropout = dp) \n",
        "            encoder_layers.append(encoder)\n",
        "            encoder_outputs, state_h2, state_c2 = encoder(encoder_outputs)\n",
        "            encoder_states.append([state_h2, state_c2])\n",
        "        if num_encoder_layers > 2:\n",
        "            encoder = LSTM(hidden_layer_size,return_sequences=True,return_state=True, dropout = dp) \n",
        "            encoder_layers.append(encoder)\n",
        "            encoder_outputs, state_h3, state_c3 = encoder(encoder_outputs)\n",
        "            encoder_states.append([state_h3, state_c3])\n",
        "\n",
        "    \n",
        "    # DECODER\n",
        "\n",
        "    decoder_inputs = Input(shape=(max_decoder_seq_length,))\n",
        "    decoder_embedding = Embedding(num_decoder_tokens, input_embedding_size, trainable=True)(decoder_inputs)\n",
        "\n",
        "    # We set up our decoder to return full output sequences, and to return internal states as well. \n",
        "    # We don't use the return states in the training model, but we will use them in inference.\n",
        "    \n",
        "    decoder_layers = []\n",
        "    if cell_type == 'RNN':\n",
        "        decoder_RNN = SimpleRNN(hidden_layer_size, return_sequences=True, return_state=True, dropout = dp)\n",
        "        decoder_layers.append(decoder_RNN)\n",
        "        decoder_outputs, _ = decoder_RNN(decoder_embedding, initial_state=encoder_states[0])\n",
        "        if num_decoder_layers > 1:\n",
        "            decoder_RNN = SimpleRNN(hidden_layer_size, return_sequences=True, return_state=True, dropout = dp)\n",
        "            decoder_layers.append(decoder_RNN)\n",
        "            decoder_outputs, _  = decoder_RNN(decoder_outputs, initial_state=encoder_states[1])\n",
        "        if num_decoder_layers > 2:\n",
        "            decoder_RNN = SimpleRNN(hidden_layer_size, return_sequences=True, return_state=True, dropout = dp)\n",
        "            decoder_layers.append(decoder_RNN)\n",
        "            decoder_outputs, _  = decoder_RNN(decoder_outputs, initial_state=encoder_states[2])\n",
        "        \n",
        "    elif cell_type == 'GRU':\n",
        "        decoder_GRU = GRU(hidden_layer_size, return_sequences=True, return_state=True, dropout = dp)\n",
        "        decoder_layers.append(decoder_GRU)\n",
        "        decoder_outputs, _ = decoder_GRU(decoder_embedding, initial_state=encoder_states[0])\n",
        "        if num_decoder_layers > 1:\n",
        "            decoder_GRU = GRU(hidden_layer_size, return_sequences=True, return_state=True, dropout = dp)\n",
        "            decoder_layers.append(decoder_GRU)\n",
        "            decoder_outputs, _  = decoder_GRU(decoder_outputs, initial_state=encoder_states[1])\n",
        "        if num_decoder_layers > 2:\n",
        "            decoder_GRU = GRU(hidden_layer_size, return_sequences=True, return_state=True, dropout = dp)\n",
        "            decoder_layers.append(decoder_GRU)\n",
        "            decoder_outputs, _  = decoder_GRU(decoder_outputs, initial_state=encoder_states[2])\n",
        "      \n",
        "    else:\n",
        "        decoder_lstm = LSTM(hidden_layer_size, return_sequences=True, return_state=True, dropout = dp)\n",
        "        decoder_layers.append(decoder_lstm)\n",
        "        decoder_outputs, _ , _ = decoder_lstm(decoder_embedding, initial_state=encoder_states[0])\n",
        "        if num_decoder_layers > 1:\n",
        "            decoder_lstm = LSTM(hidden_layer_size, return_sequences=True, return_state=True, dropout = dp)\n",
        "            decoder_layers.append(decoder_lstm)\n",
        "            decoder_outputs, _ , _  = decoder_lstm(decoder_outputs, initial_state=encoder_states[1])\n",
        "        if num_decoder_layers > 2:\n",
        "            decoder_lstm = LSTM(hidden_layer_size, return_sequences=True, return_state=True, dropout = dp)\n",
        "            decoder_layers.append(decoder_lstm)\n",
        "            decoder_outputs, _ , _  = decoder_lstm(decoder_outputs, initial_state=encoder_states[2])\n",
        "       \n",
        "    decoder_attention = AdditiveAttention(name=\"decoder_attention\")\n",
        "    decoder_concat    = Concatenate(name=\"decoder_concat\")\n",
        "    context_vec, attn_weights = decoder_attention([decoder_outputs, encoder_outputs], return_attention_scores=True)\n",
        "    decoder_outputs = decoder_concat([decoder_outputs, context_vec])\n",
        "  \n",
        "    decoder_dense = TimeDistributed(Dense(num_decoder_tokens, activation=\"softmax\"))\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "    # MODEL\n",
        "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "    return model, encoder_layers, decoder_layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwCPBckEupTX"
      },
      "source": [
        "# **Inference model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvLRxv-cCoLa"
      },
      "source": [
        "def inferencing(model,num_encoder_layers,num_decoder_layers,encoder_layers,decoder_layers,cell_type, hidden_layer_size):\n",
        "    \n",
        "    # ENCODER MODEL RECONSTRUCTION \n",
        "    encoder_inputs = model.input[0]  # input_1\n",
        "    encoder_states = []\n",
        "    enc_emb = model.layers[2]     # embedding 1\n",
        "    encoder_outputs = enc_emb(encoder_inputs)\n",
        "\n",
        "    if cell_type == 'RNN' or cell_type ==\"GRU\":\n",
        "        for i in range(num_encoder_layers):\n",
        "            encoder_outputs, state_h_enc = encoder_layers[i](encoder_outputs)\n",
        "            encoder_states += [state_h_enc] \n",
        "    else:\n",
        "        for i in range(num_encoder_layers):\n",
        "            encoder_outputs, state_h_enc, state_c_enc = encoder_layers[i](encoder_outputs)\n",
        "            encoder_states += [state_h_enc, state_c_enc]   \n",
        "\n",
        "    encoder_model = Model(encoder_inputs, encoder_states + [encoder_outputs])\n",
        "\n",
        "\n",
        "    # DECODER MODEL RECONSTRUCTION\n",
        "    input_names = [[\"input_100\",\"input_101\"],[\"input_102\",\"input_103\"],[\"input_104\",\"input_105\"],\"input_106\"]\n",
        "\n",
        "    decoder_inputs = model.input[1]       # input_2\n",
        "    decoder_embedding = model.layers[3]   # embedding 2\n",
        "    decoder_outputs = decoder_embedding(decoder_inputs)\n",
        "    decoder_states = []\n",
        "    decoder_states_inputs = []\n",
        "    \n",
        "    if cell_type == 'RNN' or cell_type ==\"GRU\":\n",
        "        for i in range(num_decoder_layers):\n",
        "            decoder_states_inputs += [Input(shape=(hidden_layer_size,), name=input_names[i][0])]\n",
        "        for i in range(num_decoder_layers):\n",
        "            decoder_outputs, state_h_dec = decoder_layers[i](decoder_outputs, initial_state=decoder_states_inputs[i])\n",
        "            decoder_states += [state_h_dec]\n",
        "    else:\n",
        "        for i in range(num_decoder_layers):\n",
        "            decoder_states_inputs += [Input(shape=(hidden_layer_size,), name=input_names[i][0]), Input(shape=(hidden_layer_size,), name=input_names[i][1])]\n",
        "        j = 0\n",
        "        for i in range(num_decoder_layers):\n",
        "            decoder_outputs, state_h_dec, state_c_dec = decoder_layers[i](decoder_outputs, initial_state=decoder_states_inputs[i+j:i+j+2])\n",
        "            decoder_states += [state_h_dec , state_c_dec]\n",
        "            j += 1\n",
        "\n",
        "    att_layer = model.layers[4+2*num_encoder_layers]\n",
        "    attn_input = Input(shape=(max_encoder_seq_length,hidden_layer_size), name=input_names[-1])   \n",
        "\n",
        "    context_vec, attn_weights = att_layer([decoder_outputs, attn_input], return_attention_scores=True)\n",
        "    \n",
        "    concat_layer = model.layers[5+2*num_encoder_layers]\n",
        "    decoder_outputs = concat_layer([decoder_outputs, context_vec])\n",
        "\n",
        "    decoder_dense = model.layers[6+2*num_encoder_layers]\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "    decoder_model = Model([decoder_inputs] + decoder_states_inputs + [attn_input], [decoder_outputs] + decoder_states + [attn_weights])\n",
        "\n",
        "    return encoder_model, decoder_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKU9UeA7q75x"
      },
      "source": [
        "def decode_sequence(input_seq,encoder_model,decoder_model):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "    attn_input = states_value[-1]\n",
        "    states_value = states_value[:-1]\n",
        "    target_seq = np.zeros((1, 1)) \n",
        "    target_seq[0, 0] = target_token_index[\"\\t\"]\n",
        "    attn_weights = []\n",
        "    stop_condition = False\n",
        "    decoded_sentence = \"\"\n",
        "    while not stop_condition:\n",
        "        output_tokens = decoder_model.predict([target_seq] + states_value + [attn_input])\n",
        "        sampled_token_index = np.argmax(output_tokens[0][0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n",
        "            stop_condition = True\n",
        "\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "        states_value = output_tokens[1:-1]\n",
        "        attn_weights.append(output_tokens[-1][0][0])\n",
        "        \n",
        "    return decoded_sentence, attn_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rf15d9ABWI0"
      },
      "source": [
        "# **Fitting the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3tft1XVCoLY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb414abe-fb07-49ad-ccc2-4ff6e033c3e7"
      },
      "source": [
        "batch_size = 128        \n",
        "epochs = 7             \n",
        "input_embedding_size = 512\n",
        "hidden_layer_size = 512\n",
        "num_layers = 1\n",
        "num_encoder_layers = num_layers\n",
        "num_decoder_layers = num_layers\n",
        "dropout = 0.2\n",
        "cell_type = 'GRU'\n",
        "\n",
        "# TRAIN\n",
        "model, encoder_layers, decoder_layers = training(input_embedding_size, dropout, cell_type, hidden_layer_size, num_encoder_layers, num_decoder_layers)\n",
        "\n",
        "# COMPILE\n",
        "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "# FIT\n",
        "model.fit(\n",
        "    [encoder_input_data, decoder_input_data],\n",
        "    decoder_target_data,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    shuffle = True,\n",
        "    validation_data= ([encoder_val_input_data, decoder_val_input_data], decoder_val_target_data)\n",
        ")\n",
        "\n",
        "# encoder_model, decoder_model = inferencing(model, num_encoder_layers, num_decoder_layers, encoder_layers, decoder_layers, cell_type, hidden_layer_size)\n",
        "# correct = 0\n",
        "# n = val_devnagri.shape[0]\n",
        "# for i in range(n):\n",
        "#     input = encoder_val_input_data[i:i+1]\n",
        "#     output, attn_weights = decode_sequence(input,encoder_model, decoder_model)\n",
        "#     if output.strip() == val_devnagri[i].strip():\n",
        "#         correct += 1\n",
        "# print(\"Validation accuracy : \", correct*100/n)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9mXbfsmyk5Z"
      },
      "source": [
        "# **Hyperparameter Tuning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRazj-4M8VYI"
      },
      "source": [
        "sweep_config = {\n",
        "    'method': 'bayes',\n",
        "    'metric': {'goal': 'maximize', 'name': 'val_accuracy'},\n",
        "    'parameters': {'input_embedding_size': {'values': [128, 256, 512]},\n",
        "                   'hidden_layer_size': {'values': [128, 256, 512]},\n",
        "                   'cell_type': {'values': ['LSTM', 'RNN', 'GRU']},\n",
        "                   'num_layers': {'values': [1,2,3]},\n",
        "                   'batch_size': {'values': [128,256,512]},\n",
        "                   'dropout': {'values': [0.1, 0.2, 0.3, 0.4]}\n",
        "                }}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2t_HvCk0vBM"
      },
      "source": [
        "def train():\n",
        "    var1 = wandb.init()\n",
        "    var2 = var1.config\n",
        "    epochs = 7\n",
        "\n",
        "    model, encoder_layers, decoder_layers = training(var2.input_embedding_size, var2.dropout, var2.cell_type , var2.hidden_layer_size, var2.num_layers, var2.num_layers)\n",
        "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "    model.fit(\n",
        "        [encoder_input_data, decoder_input_data],\n",
        "        decoder_target_data,\n",
        "        batch_size=var2.batch_size,\n",
        "        epochs=epochs,\n",
        "        callbacks=[WandbCallback()]\n",
        "    )\n",
        "\n",
        "    encoder_model, decoder_model = inferencing(model,var2.num_layers, var2.num_layers,encoder_layers,decoder_layers,var2.cell_type,var2.hidden_layer_size)\n",
        "    correct = 0\n",
        "    n = val_devnagri.shape[0]\n",
        "    for i in range(n):\n",
        "        input = encoder_val_input_data[i:i+1]\n",
        "        output, attn_weights = decode_sequence(input,encoder_model, decoder_model)\n",
        "        if output.strip() == val_devnagri[i].strip():\n",
        "            correct += 1\n",
        "    wandb.log({'val_accuracy' : correct*100/n})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMR1I7lL049l"
      },
      "source": [
        "sweep_id = wandb.sweep(sweep_config, project=\"CS6910 Assignment 3\")\n",
        "wandb.agent(sweep_id, train, count=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAEJeXHt3Gze"
      },
      "source": [
        "# **Predictions on test set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QehJccbi4Atl",
        "outputId": "6bf45fda-4f16-4363-958e-601e2fcf0c78"
      },
      "source": [
        "encoder_model, decoder_model = inferencing(model, num_encoder_layers, num_decoder_layers, encoder_layers, decoder_layers, cell_type, hidden_layer_size)\n",
        "correct = 0\n",
        "predictions = []\n",
        "attentions = []\n",
        "n = test_devnagri.shape[0]\n",
        "print(\"len test \", n)\n",
        "for i in range(n):\n",
        "    input = encoder_test_input_data[i:i+1]\n",
        "    output, attn_weights = decode_sequence(input,encoder_model, decoder_model)\n",
        "    attentions.append(attn_weights)\n",
        "    if output.strip() == test_devnagri[i].strip():\n",
        "        correct += 1\n",
        "    predictions.append(output.strip())\n",
        "print(\"Test accuracy : \", correct*100/n)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrAYHcLE6J0o"
      },
      "source": [
        "# Storing predictions\n",
        "file = open('predictions_attention.csv', 'w', newline ='', encoding = 'utf-8', errors='ignore')\n",
        "  \n",
        "with file:     \n",
        "    header = ['Input', 'Prediction', 'Ground Truth']\n",
        "    writer = csv.DictWriter(file, fieldnames = header)\n",
        "    writer.writeheader()\n",
        "    for i in range(n):\n",
        "        writer.writerow({'Input' : test_english[i], 'Prediction': predictions[i], 'Ground Truth': test_devnagri[i]})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvTIB0J1Kpud"
      },
      "source": [
        "# **Attention heatmaps**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIqlheY5KpZm"
      },
      "source": [
        "fig = []\n",
        "n = 9 \n",
        "fig , axs = plt.subplots(3,3)\n",
        "fig.set_size_inches(23, 15)\n",
        "l = -1\n",
        "k = 0\n",
        "for i in range(n):\n",
        "    output = predictions[i]\n",
        "    attn_weights = attentions[i]\n",
        "    ylabel = [\"\"]\n",
        "    m = len(attn_weights)\n",
        "    chars_ = [x for x in output]\n",
        "    xlabel = [\"\"]\n",
        "    ylabel += chars_\n",
        "    xlabel += [char for char in test_english[i]]\n",
        "    \n",
        "    for j in range(m):\n",
        "        attn_weights[j] = attn_weights[j][1:len(xlabel)]\n",
        "        \n",
        "    attn_weights = attn_weights[:-1]\n",
        "    if i%3 == 0:\n",
        "        l+=1\n",
        "        k=0\n",
        "    cax = axs[l][k].matshow(np.array(attn_weights))\n",
        "    axs[l][k].set_xticklabels(xlabel)\n",
        "    xyz = FontProperties(fname = \"MANGAL.TTF\", size = 15)\n",
        "    axs[l][k].set_yticklabels(ylabel, fontproperties = xyz)\n",
        "    k+=1\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}